{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-620580205565\n",
      "INFO:sagemaker:Creating model with name: sagemaker-mxnet-2019-01-30-19-30-23-801\n",
      "INFO:sagemaker:Creating endpoint with name table-detector-2019-01-31\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-east-1:620580205565:endpoint/table-detector-2019-01-31\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-257dac86bee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.t2.medium'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mendpoint_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'table-detector-2019-01-31'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# , instance_type='local'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium') # , instance_type='local'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, tags)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, wait)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAn\u001b[0m \u001b[0mSageMaker\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mProductionVariant\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m     \"\"\"\n\u001b[1;32m    977\u001b[0m     return {\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, wait)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mwait\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0mwait\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mendpoint\u001b[0m \u001b[0mdeployment\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mreturning\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mmodel_environment_vars\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEnvironment\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mto\u001b[0m \u001b[0mset\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m             \u001b[0mmodel_vpc_config\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mVpcConfig\u001b[0m \u001b[0mset\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0;34m*\u001b[0m \u001b[0;34m'Subnets'\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0msubnet\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-east-1:620580205565:endpoint/table-detector-2019-01-31\"."
     ]
    }
   ],
   "source": [
    "# (bucket_name='sagemaker-tables-detector', key='tables-detector-data.pickle.zip')\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "from sagemaker.mxnet.model import MXNetModel,MXNetPredictor\n",
    "\n",
    "sagemaker_model = MXNetModel(model_data=None,\n",
    "                             role=role, entry_point=\"tables_detector-class-hocr-template-with_hp_hw.py\",py_version=\"py3\")\n",
    "\n",
    "\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium',  endpoint_name = 'table-detector-2019-01-31') # , instance_type='local'\n",
    "#predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium') # , instance_type='local'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference from previously deployed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  1 tables in Mock Claims 6-v2-pdfinput-sandwich-3\n",
      "[\n",
      "    {\n",
      "        \"col_count\": 5,\n",
      "        \"coordinates\": [\n",
      "            {\n",
      "                \"x1\": 449,\n",
      "                \"x2\": 5826,\n",
      "                \"y1\": 2450,\n",
      "                \"y2\": 3200\n",
      "            }\n",
      "        ],\n",
      "        \"data\": [\n",
      "            {\n",
      "                \"c\": 0,\n",
      "                \"r\": 0,\n",
      "                \"value\": \"Dates of Service (including Confinement)\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 1,\n",
      "                \"r\": 0,\n",
      "                \"value\": \"Diagnosis Code (iCD)\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 2,\n",
      "                \"r\": 0,\n",
      "                \"value\": \"Diagnosis Description\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 3,\n",
      "                \"r\": 0,\n",
      "                \"value\": \"Procedure Code\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 4,\n",
      "                \"r\": 0,\n",
      "                \"value\": \"Procedure Descriplion\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 0,\n",
      "                \"r\": 1,\n",
      "                \"value\": \"12-11-1 \"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 1,\n",
      "                \"r\": 1,\n",
      "                \"value\": \"8 1556.911 \"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 2,\n",
      "                \"r\": 1,\n",
      "                \"value\": \"Office  Vi51t \"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 3,\n",
      "                \"r\": 1,\n",
      "                \"value\": \"99203 \"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 4,\n",
      "                \"r\": 1,\n",
      "                \"value\": \"Scn  in  Urgcnt  Care \"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 0,\n",
      "                \"r\": 2,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 1,\n",
      "                \"r\": 2,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 2,\n",
      "                \"r\": 2,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 3,\n",
      "                \"r\": 2,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 4,\n",
      "                \"r\": 2,\n",
      "                \"value\": \"and  Preated \"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 0,\n",
      "                \"r\": 3,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 1,\n",
      "                \"r\": 3,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 2,\n",
      "                \"r\": 3,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 3,\n",
      "                \"r\": 3,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 4,\n",
      "                \"r\": 3,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 0,\n",
      "                \"r\": 4,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 1,\n",
      "                \"r\": 4,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 2,\n",
      "                \"r\": 4,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 3,\n",
      "                \"r\": 4,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 4,\n",
      "                \"r\": 4,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 0,\n",
      "                \"r\": 5,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 1,\n",
      "                \"r\": 5,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 2,\n",
      "                \"r\": 5,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 3,\n",
      "                \"r\": 5,\n",
      "                \"value\": \"\"\n",
      "            },\n",
      "            {\n",
      "                \"c\": 4,\n",
      "                \"r\": 5,\n",
      "                \"value\": \"~\\u2014f\"\n",
      "            }\n",
      "        ],\n",
      "        \"modified coordinates\": [\n",
      "            {\n",
      "                \"x1\": 334,\n",
      "                \"x2\": 6095,\n",
      "                \"y1\": 2405,\n",
      "                \"y2\": 3931\n",
      "            }\n",
      "        ],\n",
      "        \"row_count\": 6,\n",
      "        \"table\": 0\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import pandas as pd\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "from sagemaker.mxnet.model import MXNetModel, MXNetPredictor\n",
    "\n",
    "#predictor =MXNetPredictor(\"table-detector-2018-12-18\") # cleaning storage\n",
    "predictor =MXNetPredictor(\"table-detector-2018-12-20\") # cleaning storage + fixing tags\n",
    "predictor =MXNetPredictor(\"table-detector-2019-01-07\") # coverage table\n",
    "predictor =MXNetPredictor(\"table-detector-2019-01-21\") # secret keys removed\n",
    "predictor =MXNetPredictor(\"table-detector-2019-01-31\") # with abdo\n",
    "\n",
    "bucket = 'sagemaker-tables-detector'\n",
    "# # ======================================================================================\n",
    "# # some handwriting\n",
    "#hocr_file_name = \"Accident-Handwritten_V3-sandwich-2t.hocr\"\n",
    "#image_file_name = \"Accident-Handwritten_V3_2.tiff\"\n",
    "# # ======================================================================================\n",
    "# # all handwriting\n",
    "#hocr_file_name = \"Accident Claim - 2-pdfinput-sandwich-3.hocr\"\n",
    "#image_file_name = \"Accident Claim - 2_3.tiff\"\n",
    "\n",
    "# # again\n",
    "#hocr_file_name = \"Accident Claim - 2_3-imageinput-sandwich.hocr\"\n",
    "#image_file_name = \"Accident Claim - 2_3-magick.tiff\"\n",
    "# # ======================================================================================\n",
    "# # all printed\n",
    "#hocr_file_name = \"page1.hocr\"\n",
    "#image_file_name = \"Page 1.tiff\"\n",
    "# # ======================================================================================\n",
    "# # noisy :)\n",
    "#hocr_file_name = \"subfolder/subsubfolder/noisy example-imageinput-sandwich.hocr\"\n",
    "#image_file_name = \"subfolder/subsubfolder/noisy example-magick.tiff\"\n",
    "# # ======================================================================================\n",
    "#hocr_file_name = \"AccidentClaim-HandwrittenP3Lat-pdfinput-sandwich.hocr\"\n",
    "#image_file_name =  \"AccidentClaim-HandwrittenP3Lat_0.tiff\"\n",
    "\n",
    "#hocr_file_name = \"Image-15.hocr\"\n",
    "#image_file_name =  \"Image-15.tiff\"\n",
    "\n",
    "#hocr_file_name = \"XMLFiles/96be1cf1-b5e9-40c6-bfa8-58164e341e09$4.hocr\" \n",
    "#image_file_name = \"ResizedImages/96be1cf1-b5e9-40c6-bfa8-58164e341e09$4.tiff\"\n",
    "#bucket = \"unum-files\"\n",
    "\n",
    "#hocr_file_name = \"Accident Claim - 5-pdfinput-sandwich-3.hocr\"\n",
    "#image_file_name = \"Accident Claim - 5_3.tiff\"\n",
    "\n",
    "#hocr_file_name = \"Accident Claim - 6-pdfinput-sandwich-3.hocr\"\n",
    "#image_file_name = \"Accident Claim - 6_3.tiff\"\n",
    "\n",
    "#hocr_file_name = \"Accident Claim - 8-pdfinput-sandwich-3.hocr\"\n",
    "#image_file_name = \"Accident Claim - 8_3.tiff\"\n",
    "\n",
    "#hocr_file_name = \"Accident Claim - 7-pdfinput-sandwich-3.hocr\" # very bad document\n",
    "#image_file_name = \"Accident Claim - 7_3.tiff\"\n",
    "\n",
    "hocr_file_name = \"Accident Claim - 2-pdfinput-sandwich-3.hocr\"\n",
    "image_file_name = \"Accident Claim - 2_3.tiff\"\n",
    "\n",
    "\n",
    "#hocr_file_name = \"Accident Claim - 2-pdfinput-sandwich-3.hocr\"\n",
    "#image_file_name = \"Accident Claim - 2_3.tiff\"\n",
    "\n",
    "#hocr_file_name = \"250091970-pdfinput-sandwich-{}.hocr\".format(3)\n",
    "#image_file_name = \"250091970_{}.tiff\".format(3)\n",
    "\n",
    "#hocr_file_name = \"Confirmation of  Coverage - 1-pdfinput-sandwich.hocr\"\n",
    "#image_file_name = \"Confirmation of  Coverage - 1_0.tiff\"\n",
    "\n",
    "#hocr_file_name = \"XMLFiles/a48e585d-d12f-44ba-8598-6a4f3cf7ab65$11.hocr\" \n",
    "#image_file_name = \"ResizedImages/a48e585d-d12f-44ba-8598-6a4f3cf7ab65$11.tiff\"\n",
    "#bucket = \"unum-files\"\n",
    "\n",
    "#hocr_file_name = \"TableFile1-sand.hocr\" \n",
    "#image_file_name = \"TableFile1.tiff\"\n",
    "\n",
    "#hocr_file_name = \"TableFile2-sand.hocr\" \n",
    "#image_file_name = \"TableFile2.tiff\"\n",
    "\n",
    "\"\"\"\n",
    "Defaults\n",
    "\"loc_endpoint\": 'sagemaker-mxnet-2018-11-28-23-22-41-830'\n",
    "\"is_new_localizer\": True\n",
    "\"hw_endpoint\": 'pytorch-handwriting-ocr-2018-11-21-20-10-49-542'\n",
    "\"hp_endpoint\": 'sagemaker-mxnet-2018-11-03-23-32-01-918'\n",
    "\n",
    "\"hw_endpoint_model\": 'new'\n",
    "\"hp_endpoint_model\": 'new'\n",
    "\n",
    "\"hw_endpoint_new_api\": True\n",
    "\"hp_endpoint_new_api\": False\n",
    "\n",
    "# access keys\n",
    "\"aws_access_key_id\": ''\n",
    "\"aws_secret_access_key\": ' '\n",
    "\"\"\"\n",
    "\n",
    "bucket=\"unum-files\"\n",
    "#hocr_file_name=\"tests/Mock Claim 1-pdfinput-sandwich-3.hocr\" \n",
    "#image_file_name = \"tests/Mock Claim 1_3.tiff\" \n",
    "hocr_file_name=\"XMLFiles/Accident Claimnatasha1-pdfinput-sandwich-0.hocr\" \n",
    "image_file_name = \"ResizedImages/Accident Claimnatasha1_0.tiff\"\n",
    "\n",
    "hocr_file_name=\"XMLFiles/Accident Claimnatasha1-pdfinput-sandwich-1.hocr\" \n",
    "image_file_name = \"ResizedImages/Accident Claimnatasha1_1.tiff\"\n",
    "\n",
    "\n",
    "#hocr_file_name=\"XMLFiles/Accident Claimnatasha1-pdfinput-sandwich-2.hocr\" \n",
    "#image_file_name = \"ResizedImages/Accident Claimnatasha1_2.tiff\"\n",
    "    \n",
    "#hocr_file_name = \"XMLFiles/Mock Claim 1201-N1-pdfinput-sandwich-5.hocr\"\n",
    "#image_file_name = \"ResizedImages/Mock Claim 1201-N1_5.tiff\"\n",
    "\n",
    "#bucket=\"unum-files\"\n",
    "hocr_file_name=\"XMLFiles/a226e91b-dd28-4ce4-a1fb-be1508bcc917$4.hocr\"\n",
    "image_file_name = \"ResizedImages/a226e91b-dd28-4ce4-a1fb-be1508bcc917$4.tiff\"\n",
    "hocr_file_name=\"Mock Claims 6-v2-pdfinput-sandwich-3.hocr\"\n",
    "image_file_name = \"Mock Claims 6-v2_3.tiff\"\n",
    "#hocr_file_name=\"XMLFiles/128eaf6e-361f-4e22-b381-9f3aa6183025.hocr\" \n",
    "#image_file_name = \"ResizedImages/128eaf6e-361f-4e22-b381-9f3aa6183025.tiff\"  # ResizedImages/9e266857-1945-40ce-9012-0c58969dea19$6.tiff\n",
    "\n",
    "#hocr_file_name=\"XMLFiles/9e266857-1945-40ce-9012-0c58969dea19$4.hocr\" \n",
    "#image_file_name = \"ResizedImages/9e266857-1945-40ce-9012-0c58969dea19$4.tiff\"  # ResizedImages/9e266857-1945-40ce-9012-0c58969dea19$6.tiff\n",
    "\n",
    "#credentials = session.get_credentials()\n",
    "#  unum-files\n",
    "data = {\"bucket\": bucket,\n",
    "        \"hocr_file\": hocr_file_name,\n",
    "        \n",
    "#         \"image_file\":\"Accident Claim - 2_3-large.tiff\", # عبدو\n",
    "        \"image_file\":image_file_name, # مش عبدو\n",
    "        \n",
    "        # defaults: dont \n",
    "        \"hp_endpoint\": \"hand-printed-model-2019-01-29-1\",\n",
    "        \"hw_endpoint\": \"pytorch-handwriting-ocr-2019-01-29-02-06-44-538\",\n",
    "        \"aws_access_key_id\": None,\n",
    "\"aws_secret_access_key\": None,\n",
    "        # localizers\n",
    "        \"loc_endpoint\": \"localization-model-2019-01-29\",\"is_new_localizer\":True # new\n",
    "        #\"loc_endpoint\":'sagemaker-mxnet-2018-11-07-23-13-24-501',\"is_new_localizer\":False # old\n",
    "           \n",
    "       }\n",
    "\n",
    "\n",
    "response = predictor.predict(data)\n",
    "#print(response)\n",
    "print(\"there are \",len(response),\"tables in\",hocr_file_name.split(\".\")[0])\n",
    "print(json.dumps(response, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', 'sagemaker==1.13.0'])\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', '-U', 'opencv-python'])\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', '-U', 'pdftabextract'])\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', '-U', 'tabula-py'])\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', '-U', 'lxml'])\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', '-U', 'pillow'])\n",
    "\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', '-U', 'intervaltree==2.1.0'])\n",
    "\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', '-U', 'xmltodict'])\n",
    "import threading\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import traceback\n",
    "from collections import OrderedDict, defaultdict\n",
    "from itertools import groupby\n",
    "from string import Template\n",
    "\n",
    "import boto3\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import xmltodict\n",
    "from intervaltree import IntervalTree\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.mxnet import MXNetPredictor\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "from pdftabextract.extract import make_grid_from_positions\n",
    "from pdftabextract.geom import rectarea\n",
    "\n",
    "\n",
    "def get_bbox(title):  # x1, y1, x2, y2\n",
    "\n",
    "    return [int(v) for v in re.findall(r'(?<=bbox )(.+?)(?=;|$)', title)[0].replace(\"bbox\", \"\").strip().split()]\n",
    "\n",
    "\n",
    "def make_sure_list(datastr):\n",
    "    if type(datastr) != list:\n",
    "        datastr = [datastr]\n",
    "\n",
    "    return datastr\n",
    "\n",
    "\n",
    "class HocrDocument(object):\n",
    "    def __init__(self, hocr_path):\n",
    "        self.hocr_path = hocr_path\n",
    "        # parser = lxml.etree.XMLParser(ns_clean=True, recover=True)\n",
    "        # self.tree = lxml.etree.parse(str(self.hocr_path), parser)\n",
    "        # is_xhtml = len(self.tree.getroot().nsmap) > 0\n",
    "        # self.xpaths = {\n",
    "        #     'page': \".//xhtml:div[@class='ocr_page']\",\n",
    "        #     'line': \".//xhtml:span[@class='ocr_line']\",\n",
    "        #     'word': \".//xhtml:span[@class='ocrx_word']\"}\n",
    "        # if is_xhtml:\n",
    "        #     self.nsmap = {'xhtml': 'http://www.w3.org/1999/xhtml'}\n",
    "        # else:\n",
    "        #     self.xpaths = {k: xp.replace('xhtml:', '')\n",
    "        #                    for k, xp in self.xpaths.items()}\n",
    "        #     self.nsmap = None\n",
    "\n",
    "        self.parse_data = None\n",
    "        self.hierarchy = None\n",
    "\n",
    "    # def _parse_title(self, title):\n",
    "    #     if title is None:\n",
    "    #         return {}\n",
    "    #     return {itm.split(\" \")[0]: \" \".join(itm.split(\" \")[1:])\n",
    "    #             for itm in title.split(\"; \")}\n",
    "\n",
    "    # def _get_img_path(self, title_data):\n",
    "    #     if 'image' in title_data:\n",
    "    #         return os.path.split(title_data['image'])[-1].replace(\"\\\"\", \"\")\n",
    "\n",
    "    # def get_pages(self):\n",
    "    #     page_node_iter = self.tree.iterfind(self.xpaths['page'],\n",
    "    #                                         namespaces=self.nsmap)\n",
    "    #     for idx, page_node in enumerate(page_node_iter):\n",
    "    #         title_data = self._parse_title(page_node.attrib.get('title'))\n",
    "    #         try:\n",
    "    #             img_path = self._get_img_path(title_data)\n",
    "    #         except ValueError:\n",
    "    #             continue\n",
    "    #         if 'bbox' not in title_data:\n",
    "    #             dimensions = Image.open(img_path).size\n",
    "    #         else:\n",
    "    #             dimensions = [int(x) for x in title_data['bbox'].split()[2:]]\n",
    "    #         page_id = page_node.attrib.get('id', 'page_{:04}'.format(idx))\n",
    "    #         yield page_id, dimensions, img_path\n",
    "\n",
    "    # def get_pages_data(self):\n",
    "    #     page_node_iter = self.tree.iterfind(self.xpaths['page'],\n",
    "    #                                         namespaces=self.nsmap)\n",
    "    #     pages_data = []\n",
    "    #     for idx, page_node in enumerate(page_node_iter):\n",
    "    #         page_id = page_node.attrib.get('id', 'page_{:04}'.format(idx))\n",
    "    #         lines_per_page = []\n",
    "    #         words_per_page = []\n",
    "    #         empty_words_per_page = []\n",
    "    #\n",
    "    #         vert_per_page = []\n",
    "    #         hor_per_page = []\n",
    "    #         line_nodes = page_node.iterfind(self.xpaths['line'],\n",
    "    #                                         namespaces=self.nsmap)\n",
    "    #\n",
    "    #         for line_node in line_nodes:\n",
    "    #\n",
    "    #             word_idx_gen = (v for v in count())\n",
    "    #             if 'not_aligned' not in line_node.attrib.get('class').split():\n",
    "    #                 title_data = self._parse_title(line_node.attrib.get('title'))\n",
    "    #                 if 'bbox' in title_data:\n",
    "    #                     bbox = [float(v) for v in title_data['bbox'].split()]  # x1, y1, x2, y2\n",
    "    #                     word_nodes = line_node.iterfind(self.xpaths['word'], namespaces=self.nsmap)\n",
    "    #                     text = list(line_node.itertext())[0::2]\n",
    "    #                     y1 = bbox[1]\n",
    "    #                     y2 = bbox[3]\n",
    "    #                     word_cuts = []\n",
    "    #\n",
    "    #                     for word_node in word_nodes:\n",
    "    #                         title_data = self._parse_title(word_node.attrib.get('title'))\n",
    "    #                         word_id = next(word_idx_gen)\n",
    "    #                         if title_data:\n",
    "    #                             word_bbox = [float(v) for v in title_data['bbox'].split()]\n",
    "    #                             word_cuts.append((word_id, word_bbox[0], word_bbox[2]))\n",
    "    #                             x1_i, y1_i, x2_i, y2_i = word_bbox[0], word_bbox[1], word_bbox[2], word_bbox[3]  # left ,top,right,bottom\n",
    "    #                             w_i, h_i = x2_i - x1_i, y2_i - y1_i\n",
    "    #\n",
    "    #                             if len(text[word_id].strip()) == 0:\n",
    "    #                                 line = {\"height\": y2_i - y1_i, \"width\": x2_i - x1_i, \"top\": y1_i, \"left\": x1_i, \"bottom\": y2_i, \"right\": x2_i}\n",
    "    #                                 if w_i > h_i * 3:  # row\n",
    "    #                                     if h_i < 0.0085 * img_orig.shape[0] and w_i > 0.04 * img_orig.shape[1]:\n",
    "    #                                         # if True:\n",
    "    #                                         hor_per_page.append(line)\n",
    "    #                                     else:\n",
    "    #                                         empty_words_per_page.append(line)\n",
    "    #                                 elif h_i > w_i * 3:  # column\n",
    "    #\n",
    "    #                                     vert_per_page.append(line)\n",
    "    #\n",
    "    #                             else:\n",
    "    #                                 words_per_page.append({\"width\": x2_i - x1_i,\n",
    "    #                                                        \"height\": y2_i - y1_i,\n",
    "    #                                                        \"value\": text[word_id],\n",
    "    #                                                        \"top\": y1_i,\n",
    "    #                                                        \"left\": x1_i, \"bottom\": y2_i,\n",
    "    #                                                        \"right\": x2_i,\n",
    "    #                                                        \"topleft\": np.array([x1_i, y1_i]),\n",
    "    #                                                        \"bottomleft\": np.array([x1_i, y2_i]),\n",
    "    #                                                        \"topright\": np.array([x2_i, y1_i]),\n",
    "    #                                                        \"bottomright\": np.array([x2_i, y2_i]),\n",
    "    #                                                        \"line_code\": (int(line_node.attrib.get(\"id\").split(\"_\")[1]), int(line_node.attrib.get(\"id\").split(\"_\")[2])),\n",
    "    #                                                        \"type\": \"tesser\"\n",
    "    #                                                        })\n",
    "    #                         else:\n",
    "    #                             word_cuts.append((word_id, -1, -1, -1))\n",
    "    #\n",
    "    #                     word_cuts_dict = {idx: (x1_i, x2_i) for idx, x1_i, x2_i in word_cuts}\n",
    "    #                     sent_cuts_indexes = [[]]\n",
    "    #\n",
    "    #                     for index in range(len(word_cuts)):\n",
    "    #                         sent_cuts_indexes[-1].append(word_cuts[index][0])\n",
    "    #                         if index < len(word_cuts) - 1 and word_cuts[index + 1][1] - word_cuts[index][2] > 1.5 * (y2 - y1):\n",
    "    #                             sent_cuts_indexes.append([])\n",
    "    #\n",
    "    #                     sent_cuts = []\n",
    "    #\n",
    "    #                     if text:\n",
    "    #                         for index, group in enumerate(sent_cuts_indexes):\n",
    "    #                             x1_i, x2_i = word_cuts_dict[group[0]][0], word_cuts_dict[group[-1]][1]\n",
    "    #                             sent_text = \" \".join([text[element_idx] for element_idx in group])\n",
    "    #                             if sent_text:\n",
    "    #                                 sent_cuts.append(\n",
    "    #                                     {\"width\": x2_i - x1_i,\n",
    "    #                                      \"height\": y2 - y1,\n",
    "    #                                      \"value\": sent_text,\n",
    "    #                                      \"top\": y1,\n",
    "    #                                      \"left\": x1_i, \"bottom\": y2,\n",
    "    #                                      \"right\": x2_i,\n",
    "    #                                      \"topleft\": np.array([x1_i, y1]),\n",
    "    #                                      \"bottomleft\": np.array([x1_i, y2]),\n",
    "    #                                      \"topright\": np.array([x2_i, y1]),\n",
    "    #                                      \"bottomright\": np.array([x2_i, y2])\n",
    "    #                                      }\n",
    "    #                                 )\n",
    "    #\n",
    "    #                         lines_per_page.append((text, bbox, word_cuts, sent_cuts))\n",
    "    #\n",
    "    #         pages_data.append((page_id, lines_per_page, words_per_page, hor_per_page, vert_per_page, empty_words_per_page))\n",
    "    #     return pages_data\n",
    "\n",
    "    def parse(self):\n",
    "        if self.parse_data is None:\n",
    "            # from intervaltree import IntervalTree\n",
    "            # sentences = OrderedDict()\n",
    "            # lines = OrderedDict()\n",
    "            # words = OrderedDict()\n",
    "            # vert = OrderedDict()\n",
    "            # hors = OrderedDict()\n",
    "            # empty = OrderedDict()\n",
    "            #\n",
    "            # for (pid, lines_s, words_s, hor_s, vert_s, empty_s), (_, dimensions, _) in list(zip(self.get_pages_data(), list(self.get_pages()))):\n",
    "            #     sentences[pid] = [sentence for _, _, _, sentences in lines_s for sentence in sentences]\n",
    "            #     words[pid] = list(filter(lambda word: (word[\"width\"] * word[\"height\"]) < dimensions[1] * dimensions[0] * .05, words_s))\n",
    "            #\n",
    "            #     temp_list = []\n",
    "            #     verticals = IntervalTree()\n",
    "            #     available_sentences = set(range(len(lines_s)))\n",
    "            #     # combine lines\n",
    "            #     for _, _, _, sentences_ in lines_s:\n",
    "            #         line_top, line_bottom, line_height = 0, 0, 0\n",
    "            #         for sentence in sentences_:\n",
    "            #             line_top += sentence[\"top\"]\n",
    "            #             line_bottom += sentence[\"bottom\"]\n",
    "            #             line_height += sentence[\"height\"]\n",
    "            #\n",
    "            #         line_top, line_bottom, line_height = line_top / len(sentences_), line_bottom / len(sentences_), line_height / len(sentences_)\n",
    "            #         temp_list.append(({\"line_top\": line_top, \"line_bottom\": line_bottom, \"line_height\": line_height}, sentences_))\n",
    "            #         verticals[line_top:line_bottom] = len(temp_list) - 1\n",
    "            #\n",
    "            #     lines[pid] = []\n",
    "            #     for index in range(len(lines_s)):\n",
    "            #         current_interval = (temp_list[index][0][\"line_top\"], temp_list[index][0][\"line_bottom\"])\n",
    "            #         candidates = set(map(lambda item: item.data,\n",
    "            #                              filter(lambda item: lineIntersect((item.begin, item.end), current_interval) > .6,\n",
    "            #                                     verticals[temp_list[index][0][\"line_top\"]:temp_list[index][0][\"line_bottom\"]])\n",
    "            #                              ))\n",
    "            #         available_sentences = available_sentences - candidates\n",
    "            #         lines[pid].append([])\n",
    "            #         for candidate in candidates:\n",
    "            #             lines[pid][-1].extend(temp_list[candidate][1])\n",
    "            #\n",
    "            #     hors[pid] = hor_s\n",
    "            #     vert[pid] = vert_s\n",
    "            #     empty[pid] = empty_s\n",
    "            #\n",
    "            # self.parse_data = OrderedDict([\n",
    "            #     (pid, {'id': pid,\n",
    "            #            'height': dimensions[1],\n",
    "            #            'width': dimensions[0],\n",
    "            #            'image': img_path,\n",
    "            #            \"sentences\": sentences[pid],\n",
    "            #            'words': words[pid],\n",
    "            #            'hor_lines': hors[pid],\n",
    "            #            'vert_lines': vert[pid],\n",
    "            #            'empty': empty[pid],\n",
    "            #            'lines': lines[pid]\n",
    "            #            })\n",
    "            #     for pid, dimensions, img_path in self.get_pages()\n",
    "            # ])\n",
    "\n",
    "            h_pages = self.hierarchical_parse()\n",
    "            new_pages = OrderedDict()\n",
    "            for pid, page in h_pages.items():\n",
    "                sentences = []\n",
    "                lines = []\n",
    "                words = []\n",
    "                vert = []\n",
    "                hors = []\n",
    "                empty = []\n",
    "\n",
    "                for block in page[\"blocks\"].values():\n",
    "                    for parag in block[\"paragraphs\"].values():\n",
    "                        for line in parag[\"lines\"].values():\n",
    "\n",
    "                            line_sentences = []\n",
    "                            for word_id, word in line[\"words\"].items():\n",
    "                                x1_i, y1_i, x2_i, y2_i = word[\"bbox\"]\n",
    "                                if x2_i - x1_i < .5 * page[\"width\"] and y2_i - y1_i < .5 * page[\"height\"]:\n",
    "                                    words.append({\"width\": x2_i - x1_i,\n",
    "                                                  \"height\": y2_i - y1_i,\n",
    "                                                  \"value\": word[\"value\"],\n",
    "                                                  \"top\": y1_i,\n",
    "                                                  \"left\": x1_i, \"bottom\": y2_i,\n",
    "                                                  \"right\": x2_i,\n",
    "                                                  \"topleft\": np.array([x1_i, y1_i]),\n",
    "                                                  \"bottomleft\": np.array([x1_i, y2_i]),\n",
    "                                                  \"topright\": np.array([x2_i, y1_i]),\n",
    "                                                  \"bottomright\": np.array([x2_i, y2_i]),\n",
    "                                                  \"line_code\": (int(word_id.split(\"_\")[1]), int(word_id.split(\"_\")[2])),\n",
    "                                                  \"type\": \"tesser\"\n",
    "                                                  })\n",
    "                            for non in line[\"non words\"].values():\n",
    "                                x1_i, y1_i, x2_i, y2_i = non[\"bbox\"]\n",
    "                                empty.append({\"height\": y2_i - y1_i, \"width\": x2_i - x1_i, \"top\": y1_i, \"left\": x1_i, \"bottom\": y2_i, \"right\": x2_i})\n",
    "\n",
    "                            for hor in line[\"horizontals\"].values():\n",
    "                                x1_i, y1_i, x2_i, y2_i = hor[\"bbox\"]\n",
    "                                hors.append({\"height\": y2_i - y1_i, \"width\": x2_i - x1_i, \"top\": y1_i, \"left\": x1_i, \"bottom\": y2_i, \"right\": x2_i})\n",
    "\n",
    "                            for ver in line[\"verticals\"].values():\n",
    "                                x1_i, y1_i, x2_i, y2_i = ver[\"bbox\"]\n",
    "                                vert.append({\"height\": y2_i - y1_i, \"width\": x2_i - x1_i, \"top\": y1_i, \"left\": x1_i, \"bottom\": y2_i, \"right\": x2_i})\n",
    "\n",
    "                            for sent in line[\"sentences\"].values():\n",
    "                                x1_i, y1_i, x2_i, y2_i = sent[\"bbox\"]\n",
    "                                dict_sent = {\"width\": x2_i - x1_i,\n",
    "                                             \"height\": y2_i - y1_i,\n",
    "                                             \"value\": sent[\"value\"],\n",
    "                                             \"top\": y1_i,\n",
    "                                             \"left\": x1_i, \"bottom\": y2_i,\n",
    "                                             \"right\": x2_i,\n",
    "                                             \"topleft\": np.array([x1_i, y1_i]),\n",
    "                                             \"bottomleft\": np.array([x1_i, y2_i]),\n",
    "                                             \"topright\": np.array([x2_i, y1_i]),\n",
    "                                             \"bottomright\": np.array([x2_i, y2_i])\n",
    "                                             }\n",
    "                                sentences.append(dict_sent)\n",
    "                                line_sentences.append(dict_sent)\n",
    "\n",
    "                            lines.append(line_sentences)\n",
    "                new_pages[pid] = {'id': pid,\n",
    "                                  'height': page[\"height\"],\n",
    "                                  'width': page[\"width\"],\n",
    "                                  'image': page[\"image\"],\n",
    "\n",
    "                                  \"sentences\": sentences,\n",
    "                                  'words': words,\n",
    "                                  'hor_lines': hors,\n",
    "                                  'vert_lines': vert,\n",
    "                                  'empty': empty,\n",
    "                                  'lines': lines\n",
    "                                  }\n",
    "            self.parse_data = new_pages\n",
    "\n",
    "        return self.parse_data\n",
    "\n",
    "    def hierarchical_parse(self):\n",
    "        if self.hierarchy is None:\n",
    "            def end_the_sentences(sent_temp_list, sent_h, word_h, line_area, line_bbox):\n",
    "                sent_left, sent_top, sent_right, sent_bottom = np.inf, np.inf, -np.inf, -np.inf\n",
    "                is_not_set = True\n",
    "                sent_text = []\n",
    "                for sent_word in sent_temp_list:\n",
    "                    sent_word_bbox = word_h[sent_word][\"bbox\"]\n",
    "                    sent_word_area = (sent_word_bbox[2] - sent_word_bbox[0]) * (sent_word_bbox[3] - sent_word_bbox[1])\n",
    "                    if sent_word_area <= line_area:\n",
    "                        is_not_set = False\n",
    "                        sent_left, sent_top, sent_right, sent_bottom = min(sent_left, sent_word_bbox[0]), min(sent_top, sent_word_bbox[1]), max(sent_right, sent_word_bbox[2]), max(sent_bottom, sent_word_bbox[3])\n",
    "                    sent_text.append(word_h[sent_word][\"value\"])\n",
    "\n",
    "                if (sent_right - sent_left) > img_orig.shape[1] * .003 and (sent_bottom - sent_top) > img_orig.shape[0] * .0035:\n",
    "                    # noinspection PyTypeChecker\n",
    "                    sent_h[sent_id] = {\"bbox\": line_bbox if is_not_set else [sent_left, sent_top, sent_right, sent_bottom], \"value\": \" \".join(sent_text)}\n",
    "\n",
    "            pages_h = OrderedDict()\n",
    "            for current_page in make_sure_list(xmltodict.parse(open(self.hocr_path, encoding=\"utf-8\").read())[\"html\"][\"body\"][\"div\"]):\n",
    "\n",
    "                page_id = current_page[\"@id\"]\n",
    "                image_path = re.findall(r'(?<=image )(.+?)(?=;|$)', current_page[\"@title\"])[0].replace(\"\\\"\", \"\").replace(\"\\'\", \"\")\n",
    "                image_size = get_bbox(current_page[\"@title\"])[2:]  # [6198, 8770]\n",
    "\n",
    "                blocks_h = OrderedDict()\n",
    "                for block in make_sure_list(current_page[\"div\"]):\n",
    "                    block_id = block[\"@id\"]\n",
    "                    block_bbox = get_bbox(block[\"@title\"])\n",
    "\n",
    "                    parag_h = OrderedDict()\n",
    "\n",
    "                    for parag in make_sure_list(block[\"p\"]):\n",
    "                        parag_id = parag[\"@id\"]\n",
    "                        parag_bbox = get_bbox(parag[\"@title\"])\n",
    "\n",
    "                        line_h = OrderedDict()\n",
    "\n",
    "                        for line in make_sure_list(parag[\"span\"]):\n",
    "                            line_id = line[\"@id\"]\n",
    "                            line_bbox = get_bbox(line[\"@title\"])  # x1, y1, x2, y2\n",
    "                            line_area = (line_bbox[2] - line_bbox[0]) * (line_bbox[3] - line_bbox[1])\n",
    "                            y1, y2 = line_bbox[1], line_bbox[3]\n",
    "                            word_h = OrderedDict()\n",
    "                            e_word_h = OrderedDict()\n",
    "                            vert_h = OrderedDict()\n",
    "                            hor_h = OrderedDict()\n",
    "\n",
    "                            sent_h = OrderedDict()\n",
    "                            sent_temp_list = []\n",
    "                            sent_id = 0\n",
    "                            for word in make_sure_list(line[\"span\"]):\n",
    "                                word_id = word[\"@id\"]\n",
    "\n",
    "                                word_bbox = get_bbox(word[\"@title\"])  # x1, y1, x2, y2\n",
    "                                current_word_left = word_bbox[0]\n",
    "                                w_i, h_i = word_bbox[2] - word_bbox[0], word_bbox[3] - word_bbox[1]\n",
    "\n",
    "                                if \"#text\" in word:\n",
    "                                    word_text = word[\"#text\"]\n",
    "                                else:\n",
    "                                    if type(word.get(\"strong\")) != str and (word.get(\"strong\") is None or word[\"strong\"].get(\"em\") is None):\n",
    "                                        word_text = None\n",
    "                                    else:\n",
    "                                        if type(word.get(\"strong\")) == str:\n",
    "                                            word_text = word[\"strong\"]\n",
    "                                        else:\n",
    "                                            word_text = word[\"strong\"][\"em\"]\n",
    "\n",
    "                                if word_text is None:  # none word\n",
    "                                    if w_i > h_i * 3:  # row\n",
    "                                        if h_i < 0.0085 * img_orig.shape[0] and w_i > 0.04 * img_orig.shape[1]:\n",
    "                                            hor_h[word_id] = {\"bbox\": word_bbox}\n",
    "                                        else:\n",
    "                                            e_word_h[word_id] = {\"bbox\": word_bbox}\n",
    "                                    elif h_i > w_i * 3:  # column\n",
    "                                        vert_h[word_id] = {\"bbox\": word_bbox}\n",
    "                                else:\n",
    "\n",
    "                                    word_h[word_id] = {\"bbox\": word_bbox, \"value\": word_text}\n",
    "                                    if len(sent_temp_list) == 0:\n",
    "                                        sent_temp_list.append(word_id)\n",
    "                                    else:\n",
    "                                        last_word_right = word_h[sent_temp_list[-1]][\"bbox\"][2]\n",
    "                                        # noinspection PyTypeChecker\n",
    "                                        if current_word_left - last_word_right > 1.5 * (y2 - y1):\n",
    "                                            # end of sentence\n",
    "                                            end_the_sentences(sent_temp_list, sent_h, word_h, line_area, line_bbox)\n",
    "\n",
    "                                            # reset for the new one\n",
    "                                            sent_id += 1\n",
    "                                            sent_temp_list = []\n",
    "\n",
    "                                        sent_temp_list.append(word_id)\n",
    "\n",
    "                            if sent_temp_list:\n",
    "                                # end of last sentence\n",
    "                                end_the_sentences(sent_temp_list, sent_h, word_h, line_area, line_bbox)\n",
    "\n",
    "                            line_h[line_id] = {\"bbox\": line_bbox, \"words\": word_h, \"non words\": e_word_h, \"horizontals\": hor_h, \"verticals\": vert_h, \"sentences\": sent_h}\n",
    "\n",
    "                        parag_h[parag_id] = {\"bbox\": parag_bbox, \"lines\": line_h}\n",
    "\n",
    "                    blocks_h[block_id] = {\"bbox\": block_bbox, \"paragraphs\": parag_h}\n",
    "\n",
    "                pages_h[page_id] = {\"image\": os.path.split(image_path)[-1].replace(\"\\\"\", \"\"), \"blocks\": blocks_h, \"height\": image_size[1], \"width\": image_size[0]}\n",
    "\n",
    "                self.hierarchy = pages_h\n",
    "        return self.hierarchy\n",
    "\n",
    "    def add_from_deployed(self, page_id, json_input, predictor, is_new_api=False):\n",
    "        # map tesseract space into localizer space\n",
    "        self.parse()\n",
    "        inputs_indexes_for_predictor = []  # here I save reference to words I update from predictor\n",
    "        inputs_for_predictor = []\n",
    "\n",
    "        is_HP = os.path.split(json_input['file_name'])[1].split(\".\")[0] == 'hand_printed'\n",
    "        print(\"Downloading original pickle from localizer...\")\n",
    "        download_file(s3, json_input[\"bucket\"], os.path.split(json_input['file_name'])[1], json_input[\"file_name\"])\n",
    "\n",
    "        localization_output = unpickle(os.path.split(json_input['file_name'])[1])  #\n",
    "        localization_bboxes = list(map(lambda item: item[1], localization_output))\n",
    "\n",
    "        for i, (x1_loc_i, y1_loc_i, w_loc_i, h_loc_i) in enumerate(localization_bboxes):\n",
    "            x2_loc_i, y2_loc_i = x1_loc_i + w_loc_i, y1_loc_i + h_loc_i\n",
    "            x1_loc_i, y1_loc_i, x2_loc_i, y2_loc_i = float(x1_loc_i), float(y1_loc_i), float(x2_loc_i), float(y2_loc_i)\n",
    "\n",
    "            loc_bbox = np.array([[x1_loc_i, y1_loc_i], [x2_loc_i, y2_loc_i]])\n",
    "            localization_found = False\n",
    "            for words_index, word in enumerate(self.parse_data[page_id][\"words\"]):\n",
    "                tesseract_bbox = np.array(\n",
    "                    [[get_in_hor_loc_space(word[\"left\"]), get_in_vert_loc_space(word[\"top\"])],\n",
    "                     [get_in_hor_loc_space(word[\"right\"]), get_in_vert_loc_space(word[\"bottom\"])]])\n",
    "\n",
    "                if rectintersect(loc_bbox, tesseract_bbox) > .8 and word[\"height\"] > 50 and word[\"width\"] > 50:  # mostly, localization boxes are larger than needed\n",
    "                    word[\"type\"] = \"HP\" if is_HP else \"HW\"  # add it\n",
    "                    inputs_indexes_for_predictor.append(words_index)\n",
    "                    localization_found = True\n",
    "\n",
    "            if not localization_found:\n",
    "                # this must be in tesseract word space !!!!!!!!!!\n",
    "                x1_loc_in_tess_i, y1_loc_in_tess_i, x2_loc_in_tess_i, y2_loc_in_tess_i = \\\n",
    "                    get_in_hor_tess_space(x1_loc_i), get_in_vert_tess_space(y1_loc_i), get_in_hor_tess_space(x2_loc_i), get_in_vert_tess_space(y2_loc_i)\n",
    "\n",
    "                word = {\"width\": x2_loc_in_tess_i - x1_loc_in_tess_i,\n",
    "                        \"height\": y2_loc_in_tess_i - y1_loc_in_tess_i,\n",
    "                        \"value\": \"tttt\",\n",
    "                        \"top\": y1_loc_in_tess_i,\n",
    "                        \"left\": x1_loc_in_tess_i,\n",
    "                        \"bottom\": y2_loc_in_tess_i,\n",
    "                        \"right\": x2_loc_in_tess_i,\n",
    "                        \"topleft\": np.array([x1_loc_in_tess_i, y1_loc_in_tess_i]),\n",
    "                        \"bottomleft\": np.array([x1_loc_in_tess_i, y2_loc_in_tess_i]),\n",
    "                        \"topright\": np.array([x2_loc_in_tess_i, y1_loc_in_tess_i]),\n",
    "                        \"bottomright\": np.array([x2_loc_in_tess_i, y2_loc_in_tess_i]),\n",
    "                        \"line_code\": (0, 0),\n",
    "                        \"type\": \"HP\" if is_HP else \"HW\",\n",
    "                        \"confidence\": .3\n",
    "                        }\n",
    "                if word[\"height\"] > 50 and word[\"width\"] > 50:\n",
    "                    self.parse_data[page_id][\"words\"].append(word)\n",
    "                    inputs_indexes_for_predictor.append(len(self.parse_data[page_id][\"words\"]) - 1)\n",
    "\n",
    "        inputs_indexes_for_predictor = list(set(inputs_indexes_for_predictor))\n",
    "        for input_index_for_predictor in inputs_indexes_for_predictor:\n",
    "            # this must be in localization space !!!!!!!!!!\n",
    "            input_words_for_predictor = self.parse_data[page_id][\"words\"][input_index_for_predictor]\n",
    "            x_1, y_1, x_2, y_2 = input_words_for_predictor[\"left\"], input_words_for_predictor[\"top\"], input_words_for_predictor[\"right\"], input_words_for_predictor[\"bottom\"]\n",
    "            x_1, y_1, x_2, y_2 = get_in_hor_loc_space(x_1), get_in_vert_loc_space(y_1), get_in_hor_loc_space(x_2), get_in_vert_loc_space(y_2)\n",
    "            x_1, y_1, x_2, y_2 = int(x_1), int(y_1), int(x_2), int(y_2)\n",
    "\n",
    "            inputs_for_predictor.append((img_for_predictors[y_1:y_2, x_1:x_2].tolist(), (x_1, y_1, x_2 - x_1, y_2 - y_1)))\n",
    "\n",
    "        if is_new_api:\n",
    "            new_inputs_for_predictor = []\n",
    "            for image, bb in inputs_for_predictor:\n",
    "                new_inputs_for_predictor.append({\"bbox\": {\"left\": bb[0], \"top\": bb[1], \"height\": bb[3], \"width\": bb[2]}, \"lines\": [image]})\n",
    "            inputs_for_predictor = new_inputs_for_predictor\n",
    "\n",
    "        print(\"Uploading refined pickle pickle from localizer to {}..\".format(json_input[\"file_name\"]))\n",
    "        upload_file(s3, json_input[\"bucket\"], pickle.dumps(inputs_for_predictor, protocol=2), json_input[\"file_name\"])\n",
    "\n",
    "        print(\"Calling the predictor..\")\n",
    "        tb = None\n",
    "        try:\n",
    "            json_predictions = predictor.predict(json_input)\n",
    "        except Exception as ex:\n",
    "            tb = traceback.format_exc()\n",
    "            raise Exception()\n",
    "        print(\"Fuckin :\",tb)\n",
    "        print(json_predictions)\n",
    "        json_predictions = json_predictions[\"result\"]\n",
    "        print(json_predictions)\n",
    "\n",
    "        if is_new_api:\n",
    "            new_json_predictions = []\n",
    "            for json_prediction in json_predictions:\n",
    "                print(\"helloo\")\n",
    "                new_json_predictions.append({'text': json_prediction[\"lines\"][0][\"text\"], 'score': json_prediction[\"lines\"][0][\"score\"], 'type of text': json_prediction[\"lines\"][0][\"type of text\"], 'y': json_prediction[\"bbox\"][\"top\"], 'w': json_prediction[\"bbox\"]['width'], 'x': json_prediction[\"bbox\"][\"left\"], 'h': json_prediction[\"bbox\"][\"height\"]})\n",
    "            json_predictions = new_json_predictions\n",
    "        for i, json_prediction in enumerate(json_predictions):\n",
    "            word_index = inputs_indexes_for_predictor[i]\n",
    "            x1_i, y1_i, x2_i, y2_i = get_in_hor_tess_space(json_prediction[\"x\"]), get_in_vert_tess_space(json_prediction[\"y\"]), \\\n",
    "                                     get_in_hor_tess_space(json_prediction[\"x\"] + json_prediction[\"w\"]), get_in_vert_tess_space(json_prediction[\"y\"] + json_prediction[\"h\"])\n",
    "\n",
    "            x1_i, y1_i, x2_i, y2_i = float(x1_i), float(y1_i), float(x2_i), float(y2_i)\n",
    "            # if json_prediction[\"score\"] > .3:\n",
    "            self.parse_data[page_id][\"words\"][word_index] = {\"width\": x2_i - x1_i,\n",
    "                                                             \"height\": y2_i - y1_i,\n",
    "                                                             \"value\": json_prediction[\"text\"],\n",
    "                                                             \"top\": y1_i,\n",
    "                                                             \"left\": x1_i,\n",
    "                                                             \"bottom\": y2_i,\n",
    "                                                             \"right\": x2_i,\n",
    "                                                             \"topleft\": np.array([x1_i, y1_i]),\n",
    "                                                             \"bottomleft\": np.array([x1_i, y2_i]),\n",
    "                                                             \"topright\": np.array([x2_i, y1_i]),\n",
    "                                                             \"bottomright\": np.array([x2_i, y2_i]),\n",
    "                                                             \"line_code\": (0, 0),\n",
    "                                                             \"type\": json_prediction[\"type of text\"],\n",
    "                                                             \"confidence\": json_prediction[\"score\"]\n",
    "                                                             }\n",
    "        print(\"Done:Added data from predictor..\")\n",
    "\n",
    "    def write_equivalent_xml(self):\n",
    "        HEADER = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "        <!DOCTYPE pdf2xml SYSTEM \"pdf2xml.dtd\">\n",
    "        <pdf2xml producer=\"poppler\" version=\"0.62.0\">\n",
    "        {PAGES}\n",
    "        </pdf2xml>\"\"\"\n",
    "        PAGE = Template(\n",
    "            \"\"\"<page number=\"1\" top=\"0\" left=\"0\" height=\"${height}\" width=\"${width}\">\n",
    "            <image top=\"0\" left=\"0\" width=\"${width}\" height=\"${height}\" src=\"${image}\"/>\n",
    "            ${TEXTS}\n",
    "            </page>\"\"\")\n",
    "\n",
    "        TEXT = \"\"\"<text top=\"{top}\" left=\"{left}\" width=\"{width}\" height=\"{height}\" >{value}</text>\"\"\"\n",
    "        data = self.parse()\n",
    "        pages_words = []\n",
    "        pages_sents = []\n",
    "\n",
    "        for pid in data:\n",
    "            page = data[pid]\n",
    "\n",
    "            sents = []\n",
    "            words = []\n",
    "            for text in page[\"sentences\"]:\n",
    "                sents.append(TEXT.format(top=text[\"top\"], left=text[\"left\"], width=text[\"width\"], height=text[\"height\"], value=text[\"value\"]))\n",
    "\n",
    "            for text in page[\"words\"]:\n",
    "                words.append(TEXT.format(top=text[\"top\"], left=text[\"left\"], width=text[\"width\"], height=text[\"height\"], value=text[\"value\"]))\n",
    "\n",
    "            pages_sents.append(PAGE.safe_substitute({\"width\": page[\"width\"], \"height\": page[\"height\"], \"image\": page[\"image\"], \"TEXTS\": \"\\n\".join(sents)}))\n",
    "            pages_words.append(PAGE.safe_substitute({\"width\": page[\"width\"], \"height\": page[\"height\"], \"image\": page[\"image\"], \"TEXTS\": \"\\n\".join(words)}))\n",
    "\n",
    "        xml_words = HEADER.format(PAGES=\"\\n\".join(pages_words))\n",
    "        xml_sents = HEADER.format(PAGES=\"\\n\".join(pages_sents))\n",
    "        open(\"test.xml\", \"w\").write(xml_words)\n",
    "\n",
    "\n",
    "page_scaling_vert = None\n",
    "page_scaling_hor = None\n",
    "\n",
    "xmlroot = None\n",
    "\n",
    "\n",
    "def get_vertical_separators(line):\n",
    "    vertical_separators = []\n",
    "    for index in range(len(line) + 1):\n",
    "\n",
    "        if index == 0:\n",
    "            separator = line[index][\"left\"] / 2\n",
    "        elif index == len(line):\n",
    "            separator = (line[index - 1][\"right\"] + img_orig.shape[1]) / 2\n",
    "        else:\n",
    "            separator = (line[index - 1][\"left\"] + line[index][\"right\"]) / 2\n",
    "        vertical_separators.append(separator)\n",
    "\n",
    "    return vertical_separators\n",
    "\n",
    "\n",
    "def is_alignment_suitable(line, current_separators):\n",
    "    # len(line) == len(summed_vertical_seperators) + 1 this is true\n",
    "    for index in range(len(line) + 1):\n",
    "        if index == 0:\n",
    "            if current_separators[index] >= line[index][\"left\"]:\n",
    "                return False\n",
    "        elif index == len(line):\n",
    "            if line[index - 1][\"right\"] >= current_separators[index]:\n",
    "                return False\n",
    "        else:\n",
    "            if not line[index - 1][\"right\"] < current_separators[index] < line[index][\"left\"]:\n",
    "                return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "class TableDetector:\n",
    "\n",
    "    def __init__(self, verbose=False, strip_height=50, w_max_pool=50, min_col_width=50, ratio_clip_max=0.8):\n",
    "        self.verbose = verbose\n",
    "        self.state = 'Table_Search'\n",
    "        self.state_machine = {'Table_Search': self.table_search,\n",
    "                              'Candidate_Table': self.candidate_table,\n",
    "                              'Table_Registered': self.table_registered,\n",
    "                              'Confirm_Table_End': self.confirm_table_end}\n",
    "        self.tables = []\n",
    "        self.tables_df = []\n",
    "        self.reset_table_info()\n",
    "        self.strip_height = strip_height\n",
    "        self.w_max_pool = w_max_pool\n",
    "        self.min_col_width = min_col_width\n",
    "        self.ratio_clip_max = ratio_clip_max\n",
    "\n",
    "    def reset_table_info(self):\n",
    "        self.table_info = {}\n",
    "        self.col_positions = []\n",
    "\n",
    "    def table_search(self):\n",
    "        '''\n",
    "        if(self.verbose):\n",
    "            print('Table_Search')\n",
    "        '''\n",
    "        if len(self.col_positions) > 0:\n",
    "            self.state = 'Candidate_Table'\n",
    "            self.table_info['table_start'] = self.start\n",
    "            self.table_info['col_positions'] = self.col_positions\n",
    "            self.n_cols = len(self.col_positions)\n",
    "\n",
    "    def candidate_table(self):\n",
    "        '''\n",
    "        if(self.verbose):\n",
    "            print('Candidate_Table')\n",
    "        '''\n",
    "        if len(self.col_positions) > 0:  # and len(self.col_positions) == self.n_cols:\n",
    "            self.state = 'Table_Registered'\n",
    "        else:\n",
    "            self.state = 'Table_Search'\n",
    "            self.reset_table_info()\n",
    "\n",
    "    def table_registered(self):\n",
    "        '''\n",
    "        if(self.verbose):\n",
    "            print('Table_Registered')\n",
    "        '''\n",
    "        if len(self.col_positions) == 0:  # or len(self.col_positions) != self.n_cols:\n",
    "            # if len(self.col_positions) == 0:\n",
    "            self.table_info['table_end'] = self.start\n",
    "            self.state = 'Confirm_Table_End'\n",
    "\n",
    "    def confirm_table_end(self):\n",
    "        '''\n",
    "        if(self.verbose):\n",
    "            print('Confirm_Table_End')\n",
    "        '''\n",
    "        if len(self.col_positions) == 0:  # or len(self.col_positions) != self.n_cols:\n",
    "            # if len(self.col_positions) != self.n_cols:\n",
    "            self.state = 'Table_Search'\n",
    "\n",
    "            self.tables.append(self.table_info)\n",
    "            self.reset_table_info()\n",
    "        else:\n",
    "            self.state = 'Table_Registered'\n",
    "\n",
    "    def remove_false_cols(self, grads):\n",
    "        # self.min_col_width = 50\n",
    "        # Get all posititions of col starts\n",
    "\n",
    "        col_starts = np.squeeze(np.argwhere(grads == 1))\n",
    "        # If dist between 2 1's < min_col_width--> set all to zeros until next 1 pos\n",
    "        prev_pos = col_starts[0]\n",
    "        for idx, pos in enumerate(col_starts):\n",
    "            if idx > 0:\n",
    "                dist = pos - prev_pos\n",
    "\n",
    "                if dist < self.min_col_width:\n",
    "                    grads[prev_pos] = 0\n",
    "                # else:\n",
    "                prev_pos = pos\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def remove_outliers(self, arr):\n",
    "        arr[np.abs(arr - np.mean(arr)) > (3 * np.std(arr))] = 0\n",
    "        return arr\n",
    "\n",
    "    def remove_consecutive(self, input):\n",
    "        # Get 1's pos\n",
    "        ones = np.squeeze(np.argwhere(input == 1))\n",
    "        # Get -1's pos\n",
    "        neg_ones = np.squeeze(np.argwhere(input == -1))\n",
    "        # Alternate from 1's and -1's. Always start by 1's\n",
    "        result = []\n",
    "        positive = True\n",
    "        next_pos = 0\n",
    "        for i in range(len(ones)):\n",
    "            if positive:\n",
    "                result.append(ones[next_pos])\n",
    "                positive = False\n",
    "                curr_pos = next_pos\n",
    "\n",
    "                next_pos = np.squeeze(np.argwhere(neg_ones > ones[curr_pos])).tolist()  # Alternate to the pos in neg_ones > curr_pos value in ones\n",
    "                if isinstance(next_pos, list):\n",
    "                    if len(next_pos) > 0:\n",
    "                        next_pos = next_pos[0]\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    next_pos = next_pos\n",
    "            else:\n",
    "                result.append(neg_ones[next_pos])\n",
    "                positive = True\n",
    "                curr_pos = next_pos\n",
    "\n",
    "                next_pos = np.squeeze(np.argwhere(ones > neg_ones[curr_pos])).tolist()\n",
    "                if isinstance(next_pos, list):\n",
    "                    if len(next_pos) > 0:\n",
    "                        next_pos = next_pos[0]\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    next_pos = next_pos\n",
    "\n",
    "        mask = np.zeros(len(input), dtype=int)\n",
    "        if (len(result) > 0):\n",
    "            mask[np.array(result)] = input[np.array(result)]\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def maxpool1D(self, h, w):\n",
    "        max_thresh = 3\n",
    "        n_w = int(len(h) / w)\n",
    "        h_maxes = np.zeros(len(h))\n",
    "        for i in range(n_w):\n",
    "\n",
    "            # h_maxes[i*w:(i+1)*w] = np.mean(h[i*w:(i+1)*w])\n",
    "            # h_maxes[i*w:(i+1)*w] = max(h[i*w:(i+1)*w])\n",
    "\n",
    "            local_max = max(h[i * w:(i + 1) * w])\n",
    "            if local_max > max_thresh:\n",
    "                h_maxes[i * w:(i + 1) * w] = local_max\n",
    "            else:\n",
    "                h_maxes[i * w:(i + 1) * w] = 0\n",
    "\n",
    "        return h_maxes\n",
    "\n",
    "    def zero_crossings(self, h_maxes):\n",
    "        res = 0 * h_maxes\n",
    "\n",
    "        pos = h_maxes > 0\n",
    "        pos2neg_positions = (~pos[:-1] & pos[1:]).nonzero()[0]\n",
    "        neg2pos_positions = (pos[:-1] & ~pos[1:]).nonzero()[0]\n",
    "        res[pos2neg_positions] = 1\n",
    "        res[neg2pos_positions] = -1\n",
    "        return res\n",
    "\n",
    "    def clean_grads(self, grads):\n",
    "        # TODO: outliers removal\n",
    "        # grads[np.abs(grads-np.mean(grads)) > (3*np.std(grads))] = 0\n",
    "        # grads = self.remove_outliers(grads)\n",
    "\n",
    "        # Adaptive threshold = max*ratio\n",
    "        thresh = np.max(grads) * self.ratio_clip_max\n",
    "        global_thresh = 2\n",
    "\n",
    "        # grads[grads < global_thresh] = 0\n",
    "\n",
    "        # Filter pos values\n",
    "        filter_pos_idx = np.squeeze(np.argwhere(np.logical_and((grads > 0), (grads <= thresh))))\n",
    "        grads[filter_pos_idx] = 0\n",
    "\n",
    "        # Filter neg values\n",
    "        filter_neg_idx = np.squeeze(np.argwhere(np.logical_and((grads < 0), (grads >= -thresh))))\n",
    "        grads[filter_neg_idx] = 0\n",
    "\n",
    "        # Normalize thr grads to 1/-1\n",
    "        grads[grads < 0] = -1\n",
    "        grads[grads > 0] = 1\n",
    "\n",
    "        # Remove consecutive 1's or -1's\n",
    "        grads = self.remove_consecutive(grads)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def preprocess_row(self, img_strip):\n",
    "\n",
    "        # 1. Histo projection on columns\n",
    "        h_all = np.sum(img_strip, axis=0)\n",
    "        h_all = self.remove_outliers(h_all)\n",
    "        # 2. Maxpool1D\n",
    "        h_maxes = self.maxpool1D(h_all, w=self.w_max_pool)\n",
    "        # Clip small maxes\n",
    "        h_maxes[h_maxes <= max(h_maxes) * self.ratio_clip_max] = 0\n",
    "\n",
    "        # h_maxes = self.remove_outliers(h_maxes)\n",
    "        # 3. Gradients\n",
    "        '''\n",
    "        g = np.gradient(h_maxes)\n",
    "        # 4. Clean grads\n",
    "        g_clean = self.clean_grads(g.copy())   \n",
    "        '''\n",
    "        g_clean = self.zero_crossings(copy.copy(h_maxes))\n",
    "        # g_clean = g\n",
    "        if self.verbose:\n",
    "            self.h = copy.copy(h_maxes)\n",
    "            # self.g = g_clean.copy()\n",
    "            self.g = copy.copy(g_clean)\n",
    "\n",
    "        return g_clean\n",
    "\n",
    "    def check_row_pattern(self, row_grads):\n",
    "        # self.g = 0*row_grads.copy()\n",
    "        if len(np.argwhere(row_grads == 1)) >= 2:  # +v crossing\n",
    "\n",
    "            # If dist between 2 1's < min_col_width--> set all to zeros\n",
    "            row_grads = self.remove_false_cols(row_grads)\n",
    "            # self.g = row_grads.copy()\n",
    "            # Count 1's => 2\n",
    "            # col_positions = np.squeeze(np.argwhere(row_grads == 1))  # row_grads are now clean\n",
    "            col_positions = list(np.argwhere(row_grads == 1)[:, 0])\n",
    "        else:\n",
    "            col_positions = []\n",
    "\n",
    "        return col_positions\n",
    "\n",
    "    def row_pattern_detect(self, img_strip):\n",
    "        g_clean = self.preprocess_row(img_strip)\n",
    "        col_positions = self.check_row_pattern(g_clean)  # clear noisy columns\n",
    "\n",
    "        self.col_positions = copy.copy(col_positions)\n",
    "\n",
    "        return col_positions\n",
    "\n",
    "    def adjust_tables_boundaries(self, img):\n",
    "        H, W = img.shape\n",
    "\n",
    "        for table in self.tables:\n",
    "            # table['table_start'] = table['table_start'] - self.strip_height\n",
    "            # table['table_end'] = table['table_end'] - self.strip_height\n",
    "            last_col = W - table['col_positions'][0]\n",
    "            table['col_positions'] = np.append(table['col_positions'], last_col)\n",
    "\n",
    "        return self.tables\n",
    "\n",
    "    def detect_tables(self, img):\n",
    "\n",
    "        H, W = img.shape\n",
    "        n_strips = int(np.floor(H / self.strip_height))\n",
    "        # overlap = 0.5\n",
    "\n",
    "        for i in range(n_strips):\n",
    "            self.start = i * self.strip_height\n",
    "            self.end = self.start + self.strip_height\n",
    "            img_strip = img[self.start:self.end, :]\n",
    "            col_positions = self.row_pattern_detect(img_strip)\n",
    "            self.state_machine[self.state]()\n",
    "\n",
    "        self.tables = self.adjust_tables_boundaries(img)\n",
    "\n",
    "        return self.tables\n",
    "\n",
    "    def visualize_tables(self):\n",
    "        tables = self.tables\n",
    "        tables_img = img_orig\n",
    "        line_width = 10\n",
    "        # fig = plt.figure()\n",
    "        for table in tables:\n",
    "            # table_start = table['table_start'] - self.strip_height\n",
    "            # table_end = table['table_end'] - self.strip_height\n",
    "            table_start = table['table_start']\n",
    "            table_end = table['table_end']\n",
    "            # Last colomn boundary = min(img_boundary, last_col_start + spacing of the previous colomn)\n",
    "            # last_col = min(tables_img.shape[1], table['col_positions'][-1] + (table['col_positions'][-1] - table['col_positions'][-2]))\n",
    "            # last_col = tables_img.shape[1] - table['col_positions'][0]\n",
    "            last_col = table['col_positions'][-1]\n",
    "            # Draw table boundaries\n",
    "            tables_img[table_start - line_width:table_start + line_width, table['col_positions'][0]: last_col] = 0\n",
    "            tables_img[table_end - line_width:table_end + line_width, table['col_positions'][0]: last_col] = 0\n",
    "            # Draw cols\n",
    "            for col in table['col_positions']:\n",
    "                tables_img[table_start:table_end, col - line_width:col + line_width] = 0\n",
    "            # Left boundary\n",
    "            tables_img[table_start:table_end, last_col - line_width:last_col + line_width] = 0\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # plt.imshow(tables_img, cmap='gray')\n",
    "        # # plt.imsave(file_name + '_table.jpg', tables_img, cmap='gray')\n",
    "        # plt.show()\n",
    "\n",
    "    def layout_based_borderless_detection(self):\n",
    "\n",
    "        mask = np.zeros_like(img_orig)\n",
    "        all_tables = set()\n",
    "        # interval tree format\n",
    "        sentences_tree = interval_format(page[\"sentences\"])\n",
    "        hor_tree = interval_format(page[\"hor_lines\"])\n",
    "        vert_tree = interval_format(page[\"vert_lines\"])\n",
    "        empt_tree = interval_format(page[\"empty\"])\n",
    "\n",
    "        # find candidate blocks\n",
    "        for block in page_h[\"blocks\"].values():\n",
    "            ignore_block = True\n",
    "\n",
    "            for par in block[\"paragraphs\"].values():\n",
    "                for line in par[\"lines\"].values():\n",
    "                    if len(line[\"sentences\"]) > 1:\n",
    "                        ignore_block = False\n",
    "\n",
    "            if not ignore_block:\n",
    "                x1, y1, x2, y2 = block[\"bbox\"]\n",
    "\n",
    "                x1, y1, x2, y2 = .95 * x1, .95 * y1, 1.05 * x2, 1.05 * y2\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "                candidate_statements = list(filter(lambda candidate: page[\"sentences\"][candidate][\"width\"] < .65 * (x2 - x1), _2d_search(x1, x2, y1, y2, tree=sentences_tree)))\n",
    "\n",
    "                candidate_hors = _2d_search(x1, x2, y1, y2, tree=hor_tree, is_strict=False)\n",
    "                candidate_vert = _2d_search(x1, x2, y1, y2, tree=vert_tree, is_strict=False)\n",
    "                candidate_empt = _2d_search(x1, x2, y1, y2, tree=empt_tree, is_strict=False)\n",
    "\n",
    "                for candidate in candidate_statements:\n",
    "                    sent = page[\"sentences\"][candidate]\n",
    "                    mask[sent[\"top\"]:sent[\"bottom\"], sent[\"left\"]:sent[\"right\"]] = 1.0\n",
    "\n",
    "                    candidate_hors -= _2d_search(sent[\"left\"], sent[\"right\"], sent[\"top\"], sent[\"bottom\"], tree=hor_tree)\n",
    "                    candidate_vert -= _2d_search(sent[\"left\"], sent[\"right\"], sent[\"top\"], sent[\"bottom\"], tree=vert_tree)\n",
    "                    candidate_empt -= _2d_search(sent[\"left\"], sent[\"right\"], sent[\"top\"], sent[\"bottom\"], tree=empt_tree)\n",
    "\n",
    "                cols = intervalize(np.sum(mask[y1:y2, x1:x2], axis=0), img_orig.shape[1] * .01, x1)\n",
    "                cols = [x1] + cols + [x2]\n",
    "                rows = intervalize(np.sum(mask[y1:y2, x1:x2], axis=1), img_orig.shape[0] * .008, y1)\n",
    "                rows = [y1] + rows + [y2]\n",
    "\n",
    "                cells = make_grid_from_positions(cols, rows)\n",
    "\n",
    "                reject = False\n",
    "                table_structure = []\n",
    "                number_of_texts = 0\n",
    "                for row, cells_row in enumerate(cells):\n",
    "                    table_structure.append([])\n",
    "                    for col, cell_bbox in enumerate(cells_row):\n",
    "                        left, top, right, bottom = cell_bbox[0, 0], cell_bbox[0, 1], cell_bbox[1, 0], cell_bbox[1, 1]\n",
    "                        cell_text_list = [page[\"sentences\"][candidate] for candidate in _2d_search(left, right, top, bottom, tree=sentences_tree)]\n",
    "\n",
    "                        lefts = [text[\"left\"] for text in cell_text_list]\n",
    "                        if lefts:\n",
    "                            shift = min(lefts)\n",
    "                        else:\n",
    "                            shift = 0\n",
    "                        groups = defaultdict(list)\n",
    "                        for key, group in groupby(cell_text_list, lambda x: int(round((x[\"left\"] - shift) / 25))):\n",
    "                            for thing in group:\n",
    "                                groups[key].append(thing)\n",
    "\n",
    "                        if len(groups) > 2 or len(candidate_hors) + len(candidate_vert) + len(candidate_empt) > .15 * len(candidate_statements):\n",
    "                            reject = True\n",
    "                        number_of_texts += len(groups)\n",
    "                        table_structure[row].append(\"\".join([text[\"value\"] for text in cell_text_list]))\n",
    "\n",
    "                for candidate in candidate_hors:\n",
    "                    if page[\"hor_lines\"][candidate][\"width\"] > .2 * (x2 - x1):\n",
    "                        reject = True\n",
    "                        break\n",
    "\n",
    "                for candidate in candidate_vert:\n",
    "                    if page[\"vert_lines\"][candidate][\"height\"] > .2 * (y2 - y1):\n",
    "                        reject = True\n",
    "                        break\n",
    "\n",
    "                if not reject:\n",
    "                    # clean noisy columns/rows\n",
    "                    remove = True\n",
    "                    for col in table_structure[0]:\n",
    "                        if col != \"\":\n",
    "                            remove = False\n",
    "\n",
    "                    if remove:\n",
    "                        rows = [rows[0]] + rows[2:]\n",
    "                        table_structure = table_structure[1:]\n",
    "\n",
    "                    remove = True\n",
    "                    for col in table_structure[-1]:\n",
    "                        if col != \"\":\n",
    "                            remove = False\n",
    "\n",
    "                    if remove:\n",
    "                        rows = rows[:-2] + [rows[-1]]\n",
    "                        table_structure = table_structure[:-1]\n",
    "\n",
    "                    remove = True\n",
    "                    for row in table_structure:\n",
    "                        if row[0] != \"\":\n",
    "                            remove = False\n",
    "\n",
    "                    if remove:\n",
    "                        cols = cols[:-2] + [cols[-1]]\n",
    "                        for i in range(len(table_structure)):\n",
    "                            table_structure[i] = table_structure[i][0:]\n",
    "\n",
    "                    remove = True\n",
    "                    for row in table_structure:\n",
    "                        if row[-1] != \"\":\n",
    "                            remove = False\n",
    "                    if remove:\n",
    "                        cols = cols[:-2] + [cols[-1]]\n",
    "                        for i in range(len(table_structure)):\n",
    "                            table_structure[i] = table_structure[i][:-1]\n",
    "\n",
    "                    pd_table = pd.DataFrame(table_structure)\n",
    "                    reject = pd_table.shape == (1, 1) or number_of_texts > 2 * pd_table.shape[0] * pd_table.shape[1] or 2 * number_of_texts < pd_table.shape[0] * pd_table.shape[1] or cols[-1] - cols[0] < .5 * page[\"width\"]\n",
    "                    if not reject:\n",
    "                        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "                            # print(len(candidate_hors) + len(candidate_vert) + len(candidate_empt), len(candidate_statements))\n",
    "                            # print(pd_table, pd_table.shape)\n",
    "                            if str(pd_table) not in all_tables:\n",
    "                                self.tables_df.append(pd_table)\n",
    "                                self.tables.append({\"col_positions\": cols, \"table_start\": rows[0], \"table_end\": rows[-1], \"modified_left\": cols[0], \"modified_top\": rows[0], \"modified_right\": cols[-1], \"modified_bottom\": rows[-1]})\n",
    "                                all_tables.add(str(pd_table))\n",
    "                                # #######################################\n",
    "                                # img_orig_local = cv2.cvtColor(img_orig.copy(), cv2.COLOR_GRAY2BGR)\n",
    "                                # for candidate in candidate_statements:\n",
    "                                #     sent = page[\"sentences\"][candidate]\n",
    "                                #     cv2.rectangle(img_orig_local, (sent[\"left\"], sent[\"top\"]), (sent[\"right\"], sent[\"bottom\"]), (255, 0, 0), 6)\n",
    "                                #\n",
    "                                # for candidate in candidate_vert:\n",
    "                                #     sent = page[\"vert_lines\"][candidate]\n",
    "                                #     cv2.rectangle(img_orig_local, (sent[\"left\"], sent[\"top\"]), (sent[\"right\"], sent[\"bottom\"]), (0, 255, 0), 6)\n",
    "                                #\n",
    "                                # for candidate in candidate_hors:\n",
    "                                #     sent = page[\"hor_lines\"][candidate]\n",
    "                                #     cv2.rectangle(img_orig_local, (sent[\"left\"], sent[\"top\"]), (sent[\"right\"], sent[\"bottom\"]), (0, 0, 255), 6)\n",
    "                                #\n",
    "                                # for candidate in candidate_empt:\n",
    "                                #     sent = page[\"empty\"][candidate]\n",
    "                                #     cv2.rectangle(img_orig_local, (sent[\"left\"], sent[\"top\"]), (sent[\"right\"], sent[\"bottom\"]), (255, 0, 255), 6)\n",
    "                                #\n",
    "                                # print(candidate_vert, candidate_hors, candidate_empt)\n",
    "                                # for row in rows:\n",
    "                                #     cv2.line(img_orig_local, (x1, row), (x2, row), (0, 0, 128), 15)\n",
    "                                # for col in cols:\n",
    "                                #     cv2.line(img_orig_local, (col, y1), (col, y2), (0, 0, 128), 15)\n",
    "                                #\n",
    "                                # cv2.namedWindow(\"table\", cv2.WINDOW_NORMAL)\n",
    "                                # cv2.imshow(\"table\", img_orig_local[y1:y2, x1:x2])\n",
    "                                # cv2.waitKey()\n",
    "                                #\n",
    "                                # #######################################\n",
    "\n",
    "    # def line_based_detection(self):\n",
    "    #     # get_vertical_separators\n",
    "    #     # is_alignment_suitable()\n",
    "    #\n",
    "    #     # mean_height = np.mean([word[\"height\"] for word in page[\"words\"]])\n",
    "    #     # threshold_height = mean_height * 2.5\n",
    "    #     summed_vertical_seperators = []\n",
    "    #     lines = []\n",
    "    #     for line in page[\"lines\"]:\n",
    "    #\n",
    "    #         is_valid = True\n",
    "    #         for sentence in line:\n",
    "    #             if sentence[\"width\"] > img_orig.shape[1] * .45:\n",
    "    #                 is_valid = False\n",
    "    #                 break\n",
    "    #         current_seperator = get_vertical_separators(line)\n",
    "    #         if is_valid:\n",
    "    #             if lines and (len(line) + 1 != len(summed_vertical_seperators) or not is_alignment_suitable(line, [ver / len(lines) for ver in summed_vertical_seperators])):  # end of table\n",
    "    #                 # get the grid\n",
    "    #                 summed_vertical_seperators = []\n",
    "    #                 lines = []\n",
    "    #             else:  # continue\n",
    "    #                 if summed_vertical_seperators:\n",
    "    #                     for i, value in enumerate(current_seperator):\n",
    "    #                         summed_vertical_seperators[i] += value\n",
    "    #                 lines.append(line)\n",
    "    #\n",
    "    #             if len(line) == 1:\n",
    "    #                 summed_vertical_seperators = []\n",
    "    #                 lines = []\n",
    "\n",
    "    # def fit_borderless_tables(self):\n",
    "    #     self.tables_df = []\n",
    "    #     tables_with_text = []\n",
    "    #     for table_index, table in enumerate(self.tables):\n",
    "    #         table['col_positions'] = sorted(table['col_positions'])\n",
    "    #\n",
    "    #         table_top, table_bottom, table_left, table_right = table[\"table_start\"], table[\"table_end\"], table[\"col_positions\"][0], table[\"col_positions\"][-1]\n",
    "    #\n",
    "    #         page_colpos_is = table['col_positions']\n",
    "    #         # right border of the last column\n",
    "    #         last_rightborder_is = page_colpos_is[-1]\n",
    "    #\n",
    "    #         # calculate median text box height\n",
    "    #         median_text_height_ts = np.median([t_ts['height'] for t_ts in page['sentences']])\n",
    "    #\n",
    "    #         # get all texts in all the with a \"usual\" textbox height\n",
    "    #         # we will only use these text boxes in order to determine the line positions\n",
    "    #\n",
    "    #         text_height_deviation_thresh_ts = median_text_height_ts / 2\n",
    "    #         texts_cols_ts = [t_ts for t_ts in page['sentences']\n",
    "    #                          if get_in_hor_loc_space(t_ts['right']) <= last_rightborder_is  # <<<<<<<<<< scaling to image space\n",
    "    #                          and abs(t_ts['height'] - median_text_height_ts) <= text_height_deviation_thresh_ts]\n",
    "    #         # Next we get the text boxes' top and bottom border positions, cluster them, and calculate the cluster centers.\n",
    "    #\n",
    "    #         # get all textboxes' top and bottom border positions\n",
    "    #         borders_y_ts = border_positions_from_texts(texts_cols_ts, DIRECTION_VERTICAL)\n",
    "    #\n",
    "    #         # break into clusters using half of the median text height as break distance\n",
    "    #         clusters_y_ts = find_clusters_1d_break_dist(borders_y_ts, dist_thresh=median_text_height_ts / 2)\n",
    "    #         clusters_w_vals_ts = zip_clusters_and_values(clusters_y_ts, borders_y_ts)\n",
    "    #\n",
    "    #         # for each cluster, calculate the median as center\n",
    "    #         pos_y_ts = calc_cluster_centers_1d(clusters_w_vals_ts)\n",
    "    #         pos_y_ts.append(page['height'])\n",
    "    #\n",
    "    #         top_y_is = table['table_start']\n",
    "    #\n",
    "    #         bottom_y_is = table['table_end']\n",
    "    #\n",
    "    #         # finally filter the line positions so that only the lines between the table top and bottom are left\n",
    "    #         page_rowpos_ts = [y_ts for y_ts in pos_y_ts if top_y_is <= get_in_vert_loc_space(y_ts) <= bottom_y_is]\n",
    "    #\n",
    "    #         # ##################################\n",
    "    #         # print(table_index)\n",
    "    #         # draw = img_orig_words.copy()\n",
    "    #         # cv2.namedWindow(\"original\", cv2.WINDOW_NORMAL)\n",
    "    #         # for line in pos_y_ts:\n",
    "    #         #     draw = cv2.line(draw, (0, int(line)), (draw.shape[1], int(line)), (0, 255, 0), 6)\n",
    "    #         #\n",
    "    #         # draw = cv2.resize(draw, (2479, 3508))\n",
    "    #         # cv2.imshow(\"original\", draw[table_top:table_bottom, table_left:table_right])\n",
    "    #         # cv2.waitKey()\n",
    "    #         # ##################################\n",
    "    #         try:\n",
    "    #             ## 7. Create a grid of columns and lines\n",
    "    #             # From the column and row positions that we detected, we can now generate a \"page grid\" which should resemble the table layout as close as possible. We then save the grid information as JSON file so that we can display it in pdf2xml-viewer.\n",
    "    #             grid_ps = make_grid_from_positions([get_in_hor_tess_space(colpos) for colpos in page_colpos_is], page_rowpos_ts)\n",
    "    #\n",
    "    #             table[\"modified_top\"] = table_top\n",
    "    #             table[\"modified_bottom\"] = table_bottom\n",
    "    #             table[\"modified_left\"] = table_left\n",
    "    #             table[\"modified_right\"] = table_right\n",
    "    #             datatable = fit_texts_into_grid(page['sentences'], grid_ps)\n",
    "    #\n",
    "    #             df = datatable_to_dataframe(datatable)\n",
    "    #             # Rename columns\n",
    "    #             new_names = []\n",
    "    #             for i in range(len(df.columns)):\n",
    "    #                 new_names.append(str(i))\n",
    "    #\n",
    "    #             df.columns = new_names\n",
    "    #             with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    #                 print(df)\n",
    "    #\n",
    "    #             self.tables_df.append(df)\n",
    "    #             tables_with_text.append(table)\n",
    "    #\n",
    "    #         except Exception as e:\n",
    "    #\n",
    "    #             traceback.print_tb(e.__traceback__)\n",
    "    #             print('Empty table\\n')\n",
    "\n",
    "    def fit_bordered_tables(self):\n",
    "\n",
    "        self.tables_df = []\n",
    "        tables_with_text = []\n",
    "        all_tables = set()\n",
    "\n",
    "        for table_index, table in enumerate(self.tables):\n",
    "\n",
    "            table['col_positions'] = sorted(table['col_positions'])\n",
    "\n",
    "            try:\n",
    "\n",
    "                initial_cols, initial_rows, words = [], [], []\n",
    "\n",
    "                table_top, table_bottom, table_left, table_right = table[\"table_start\"], table[\"table_end\"], table[\"col_positions\"][0], table[\"col_positions\"][-1]\n",
    "\n",
    "                # ###################################\n",
    "                # print(table_index)\n",
    "                # cv2.namedWindow(\"original\", cv2.WINDOW_NORMAL)\n",
    "                # cv2.imshow(\"original\", img_orig_words[table_top:table_bottom, table_left:table_right])\n",
    "                # cv2.waitKey()\n",
    "                # ##################################\n",
    "\n",
    "                table_height, table_width = table_bottom - table_top, table_right - table_left\n",
    "                table_bbox = np.array([[table_left, table_top], [table_right, table_bottom]])\n",
    "\n",
    "                for hor in page['hor_lines']:\n",
    "                    left_i, right_i, top_i, bottom_i, width_i, height_i = int(hor[\"left\"]), int(hor[\"right\"]), int(hor[\"top\"]), int(hor[\"bottom\"]), hor[\"width\"], hor[\"height\"]\n",
    "                    hor_bbox = np.array([[left_i, top_i], [right_i, bottom_i]])\n",
    "\n",
    "                    if rectintersect(table_bbox, hor_bbox) > .3 and width_i > .25 * page[\"width\"]:\n",
    "                        initial_rows.append((left_i, right_i, (bottom_i + top_i) / 2))\n",
    "\n",
    "                for vert in page[\"vert_lines\"]:\n",
    "                    left_i, right_i, top_i, bottom_i, width_i, height_i = int(vert[\"left\"]), int(vert[\"right\"]), int(vert[\"top\"]), int(vert[\"bottom\"]), vert[\"width\"], vert[\"height\"]\n",
    "                    vert_bbox = np.array([[left_i, top_i], [right_i, bottom_i]])\n",
    "\n",
    "                    if rectintersect(table_bbox, vert_bbox) > .31 and height_i > .05 * page[\"height\"]:\n",
    "                        initial_cols.append((top_i, bottom_i, (left_i + right_i) / 2))\n",
    "\n",
    "                if initial_cols and initial_rows:\n",
    "                    table[\"modified_top\"] = modified_top = int(np.min(list(map(lambda item: item[0], initial_cols))))\n",
    "                    table[\"modified_bottom\"] = modified_bottom = int(np.max(list(map(lambda item: item[1], initial_cols))))\n",
    "                    table[\"modified_left\"] = modified_left = int(np.min(list(map(lambda item: item[0], initial_rows))))\n",
    "                    table[\"modified_right\"] = modified_right = int(np.max(list(map(lambda item: item[1], initial_rows))))\n",
    "                    table_bbox = np.array([[modified_left, modified_top], [modified_right, modified_bottom]])\n",
    "\n",
    "                    cols, rows = [], []\n",
    "                    for hor in page['hor_lines']:\n",
    "                        left_i, right_i, top_i, bottom_i, width_i, height_i = int(hor[\"left\"]), int(hor[\"right\"]), int(hor[\"top\"]), int(hor[\"bottom\"]), hor[\"width\"], hor[\"height\"]\n",
    "                        hor_bbox = np.array([[left_i, top_i], [right_i, bottom_i]])\n",
    "\n",
    "                        if rectintersect(table_bbox, hor_bbox) > .3 and width_i > .25 * page[\"width\"]:\n",
    "                            rows.append((left_i, right_i, (bottom_i + top_i) / 2))\n",
    "\n",
    "                    for vert in page[\"vert_lines\"]:\n",
    "                        left_i, right_i, top_i, bottom_i, width_i, height_i = int(vert[\"left\"]), int(vert[\"right\"]), int(vert[\"top\"]), int(vert[\"bottom\"]), vert[\"width\"], vert[\"height\"]\n",
    "                        vert_bbox = np.array([[left_i, top_i], [right_i, bottom_i]])\n",
    "\n",
    "                        if rectintersect(table_bbox, vert_bbox) > .31 and height_i > .05 * page[\"height\"]:\n",
    "                            cols.append((top_i, bottom_i, (left_i + right_i) / 2))\n",
    "\n",
    "                    for word in page[\"words\"]:\n",
    "                        left_i, right_i, top_i, bottom_i = int(word[\"left\"]), int(word[\"right\"]), int(word[\"top\"]), int(word[\"bottom\"])\n",
    "                        word_bbox = np.array([[left_i, top_i], [right_i, bottom_i]])\n",
    "                        if rectintersect(table_bbox, word_bbox) > .5:\n",
    "                            words.append(word)\n",
    "\n",
    "                    rows = sorted(list(map(lambda item: int(item[2]), rows)))\n",
    "                    cols = sorted(list(map(lambda item: int(item[2]), cols)))\n",
    "\n",
    "                    cols_new = []\n",
    "                    for i in range(len(cols)):\n",
    "                        if not cols_new or abs(cols[i] - cols[i - 1]) > 40:\n",
    "                            cols_new.append(cols[i])\n",
    "                        else:\n",
    "                            cols_new[-1] = (cols[i] + cols[i - 1]) // 2\n",
    "                    cols = cols_new\n",
    "\n",
    "                    rows_new = []\n",
    "                    for i in range(len(rows)):\n",
    "                        if not rows_new or abs(rows[i] - rows[i - 1]) > 40:\n",
    "                            rows_new.append(rows[i])\n",
    "                        else:\n",
    "                            rows_new[-1] = (rows[i] + rows[i - 1]) // 2\n",
    "                    rows = rows_new\n",
    "\n",
    "                    if modified_top < min(rows) and min(rows) - modified_top > table_height * .1:\n",
    "                        rows = [modified_top] + rows\n",
    "                    if max(rows) < modified_bottom and modified_bottom - max(rows) > table_height * .1:\n",
    "                        rows = rows + [modified_bottom]\n",
    "\n",
    "                    if modified_left < min(cols) and min(cols) - modified_left > table_width * .1:\n",
    "                        cols = [modified_left] + cols\n",
    "                    if max(cols) < modified_right and modified_right - max(cols) > table_width * .1:\n",
    "                        cols = cols + [modified_right]\n",
    "\n",
    "                    if len(cols) > 2 and len(rows) > 2:\n",
    "                        cells = make_grid_from_positions(cols, rows)\n",
    "\n",
    "                        best_word_locations = []\n",
    "                        for _ in words:\n",
    "                            best_word_locations.append((-1, -1, 0))  # row, col, score\n",
    "\n",
    "                        table_structure = []\n",
    "                        for row, cells_row in enumerate(cells):\n",
    "                            table_structure.append([])\n",
    "                            for col, cell_bbox in enumerate(cells_row):\n",
    "                                table_structure[row].append([])\n",
    "                                for word_index, word in enumerate(words):\n",
    "                                    word_bbox = np.array([[word[\"left\"], word[\"top\"]], [word[\"right\"], word[\"bottom\"]]])\n",
    "                                    inter = rectintersect(cell_bbox, word_bbox)\n",
    "                                    best_score = best_word_locations[word_index][2]\n",
    "                                    if inter > best_score:\n",
    "                                        best_word_locations[word_index] = (row, col, inter)\n",
    "\n",
    "                        for word_index, word in enumerate(words):\n",
    "                            row, col, _ = best_word_locations[word_index]\n",
    "                            if row != -1:\n",
    "                                table_structure[row][col].append(words[word_index])\n",
    "                        for row, cells_row in enumerate(cells):\n",
    "                            for col, cell_bbox in enumerate(cells_row):\n",
    "                                table_structure[row][col] = \" \".join(map(lambda item: item[\"value\"], sorted(table_structure[row][col], key=lambda item: (item[\"line_code\"][0], item[\"line_code\"][1], item[\"left\"], item[\"right\"]))))\n",
    "\n",
    "                        df = pd.DataFrame(table_structure)\n",
    "\n",
    "                        # Rename columns\n",
    "                        new_names = []\n",
    "                        for j in range(len(df.columns)):\n",
    "                            new_names.append(str(j))\n",
    "\n",
    "                        df.columns = new_names\n",
    "\n",
    "                        if self.post_process_tables(df.copy()) and str(df) not in all_tables:\n",
    "                            self.tables_df.append(df)\n",
    "                            tables_with_text.append(table)\n",
    "                            all_tables.add(str(df))\n",
    "                            print(\"Table detected:\", table_index, \"with shape\", df.shape)\n",
    "\n",
    "                    # print(df, \"\\n\", self.post_process_tables(df.copy()))\n",
    "                    # print(\"> page: grid with %d rows, %d columns\" % (n_rows, n_cols))\n",
    "            except Exception as e:\n",
    "\n",
    "                traceback.print_tb(e.__traceback__)\n",
    "                print(table_index)\n",
    "\n",
    "        # print('=' * 100)\n",
    "        self.tables = tables_with_text  # COORD\n",
    "\n",
    "    def post_process_tables(self, table_df):\n",
    "        template_header_col = {'0': ['Dates', 'Service', 'Confinement'],\n",
    "                               '1': ['Diagnosis', 'Code', 'ICD', '(ICD)'],\n",
    "                               '2': ['Diagnosis', 'Description'],\n",
    "                               '3': ['Procedure', 'Code'],\n",
    "                               '4': ['Procedure', 'Description'],\n",
    "                               }\n",
    "        header_limit = 5\n",
    "\n",
    "        result = False\n",
    "        for col in table_df.columns[:5]:\n",
    "            # col_data = table_df[col]\n",
    "            template_header = template_header_col[col]\n",
    "            for row in range(min(header_limit, len(table_df[col]))):\n",
    "                for header in template_header:\n",
    "                    if header in table_df[col][row]:\n",
    "                        result = True\n",
    "        return result\n",
    "        # '''\n",
    "        # def post_process_tables(self, table_df):\n",
    "        # template_header = ['Dates', 'Diagnosis', 'Description', 'Procedure', 'Service', 'Confinement']\n",
    "        # header_limit = 5\n",
    "        #\n",
    "        # result = False\n",
    "        # for col in table_df.columns:\n",
    "        #     #col_data = table_df[col]\n",
    "        #     for row in range(min(header_limit, len(table_df[col]))):\n",
    "        #         for header in template_header:\n",
    "        #             if header in table_df[col][row]:\n",
    "        #                 result = True\n",
    "        # return result\n",
    "        # '''\n",
    "\n",
    "    def get_json_response(self):\n",
    "        tables_structure = []\n",
    "        for i, table_df in enumerate(self.tables_df):\n",
    "            table_structure = []\n",
    "            for row_index, row_series in table_df.iterrows():\n",
    "                for col_index, data_item in enumerate(row_series):\n",
    "                    table_structure.append({\"r\": row_index, \"c\": col_index, \"value\": data_item})\n",
    "            tables_structure.append({\"table\": i,\n",
    "                                     # COORD\n",
    "                                     \"coordinates\": [{'x1': int(self.tables[i]['col_positions'][0]), 'y1': int(self.tables[i]['table_start']), 'x2': int(self.tables[i]['col_positions'][-1]), 'y2': int(self.tables[i]['table_end'])}]\n",
    "                                        , \"modified coordinates\": [{'x1': int(self.tables[i][\"modified_left\"]), 'y1': int(self.tables[i][\"modified_top\"]), 'x2': int(self.tables[i][\"modified_right\"]),\n",
    "                                                                    'y2': int(self.tables[i][\"modified_bottom\"])}]\n",
    "                                        , \"row_count\": table_df.shape[0], \"col_count\": table_df.shape[1], \"data\": table_structure})\n",
    "\n",
    "        response_body = json.dumps(tables_structure)\n",
    "\n",
    "        return response_body\n",
    "\n",
    "\n",
    "def get_in_hor_image_space(value_from_xml_hor):\n",
    "    return value_from_xml_hor * page_scaling_hor\n",
    "\n",
    "\n",
    "def get_in_vert_image_space(value_from_xml_vert):\n",
    "    return value_from_xml_vert * page_scaling_vert\n",
    "\n",
    "\n",
    "def get_in_hor_page_space(value_from_image_hor):\n",
    "    return value_from_image_hor / page_scaling_hor\n",
    "\n",
    "\n",
    "def get_in_vert_page_space(value_from_image_vert):\n",
    "    return value_from_image_vert / page_scaling_vert\n",
    "\n",
    "\n",
    "def get_in_hor_tess_space(value_from_loc_hor):\n",
    "    return value_from_loc_hor * tess_scaling_hor\n",
    "\n",
    "\n",
    "def get_in_vert_tess_space(value_from_loc_vert):\n",
    "    return value_from_loc_vert * tess_scaling_vert\n",
    "\n",
    "\n",
    "def get_in_hor_loc_space(value_from_tess_hor):\n",
    "    return value_from_tess_hor / tess_scaling_hor\n",
    "\n",
    "\n",
    "def get_in_vert_loc_space(value_from_tess_vert):\n",
    "    return value_from_tess_vert / tess_scaling_vert\n",
    "\n",
    "\n",
    "def rectintersect(a, b):\n",
    "    # normalizes with respect to the second\n",
    "    a_left, a_top, a_right, a_bottom = a[0, 0], a[0, 1], a[1, 0], a[1, 1]\n",
    "    b_left, b_top, b_right, b_bottom = b[0, 0], b[0, 1], b[1, 0], b[1, 1]\n",
    "\n",
    "    if a_right <= b_left or b_right <= a_left or b_bottom <= a_top or a_bottom <= b_top:\n",
    "        return 0\n",
    "\n",
    "    intersection_box = np.array([[max(a[0, 0], b[0, 0]), max(a[0, 1], b[0, 1])], [min(a[1, 0], b[1, 0]), min(a[1, 1], b[1, 1])]])\n",
    "\n",
    "    area = rectarea(intersection_box)\n",
    "\n",
    "    return area / rectarea(b)\n",
    "\n",
    "\n",
    "def interval_format(list_of_dict):\n",
    "    tree_hor = IntervalTree()\n",
    "    tree_ver = IntervalTree()\n",
    "\n",
    "    for i, sent in enumerate(list_of_dict):\n",
    "        tree_hor[sent[\"left\"]:sent[\"right\"]] = i\n",
    "        tree_ver[sent[\"top\"]:sent[\"bottom\"]] = i\n",
    "\n",
    "    return tree_hor, tree_ver\n",
    "\n",
    "\n",
    "def lineIntersect(a, b):\n",
    "    a_min, a_max, b_min, b_max = a[0], a[1], b[0], b[1]\n",
    "\n",
    "    intersection = max(a_min, b_min), min(b_max, a_max)\n",
    "    if intersection[1] < intersection[0]:\n",
    "        return 0\n",
    "    union = min(a_min, b_min), max(b_max, a_max)\n",
    "    return (intersection[1] - intersection[0]) / (union[1] - union[0])\n",
    "\n",
    "\n",
    "def upload_file(s3, bucket, bin_data, s3_path_to_file):\n",
    "    # s3.Bucket(bucket).upload_file(local_path_to_file, s3_path_to_file)\n",
    "    s3.Object(bucket_name=bucket, key=s3_path_to_file).put(Body=bin_data)  ## Body = some binary data protocol=2\n",
    "\n",
    "\n",
    "def download_file(s3, bucket, local_path_to_file, s3_path_to_file):\n",
    "    file_binary_stream = s3.Object(bucket_name=bucket, key=s3_path_to_file)\n",
    "    with open(local_path_to_file, 'wb') as f:\n",
    "        f.write(file_binary_stream.get()[\"Body\"].read())\n",
    "\n",
    "\n",
    "def unpickle(path_to_file):\n",
    "    with open(path_to_file, 'rb') as f:  #\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    cleaner = threading.Thread(target=lambda: os.remove(path_to_file))\n",
    "    cleaner.start()\n",
    "    return data\n",
    "\n",
    "\n",
    "def _2d_search(x1, x2, y1, y2, tree, is_strict=True):\n",
    "    tree_hor, tree_ver = tree\n",
    "    hor_candidates = set(map(lambda item: item.data, tree_hor.search(x1, x2, is_strict)))\n",
    "    ver_candidates = set(map(lambda item: item.data, tree_ver.search(y1, y2, is_strict)))\n",
    "    return set.intersection(hor_candidates, ver_candidates)\n",
    "\n",
    "\n",
    "def intervalize(sum_on_dim, thresh, shift):\n",
    "    projection = np.reshape(np.argwhere(sum_on_dim == 0), [-1])\n",
    "    intervals = []\n",
    "    if len(projection) > 0:\n",
    "        last_start = projection[0]\n",
    "        for index in range(1, projection.shape[0]):\n",
    "            if projection[index] != projection[index - 1] + 1:\n",
    "                intervals.append((last_start, projection[index - 1]))\n",
    "                last_start = projection[index]\n",
    "        intervals.append((last_start, projection[-1]))\n",
    "\n",
    "    return list(map(lambda item: shift + (item[1] + item[0]) // 2, filter(lambda item: (item[1] - item[0]) > thresh, intervals)))\n",
    "\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "def id_generator(size=12, chars=string.ascii_uppercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))\n",
    "\n",
    "\n",
    "# Initialize handwriting model\n",
    "class JSONPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(JSONPredictor, self).__init__(endpoint_name, sagemaker_session, json_serializer, json_deserializer)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------ #\n",
    "# Hosting methods                                              #\n",
    "# ------------------------------------------------------------ #\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    return None\n",
    "\n",
    "\n",
    "def transform_fn(none_model, data, input_content_type, output_content_type):\n",
    "    global page_scaling_vert, tess_scaling_vert, img_orig, page_scaling_hor, tess_scaling_hor, page, page_h, s3, img_for_predictors\n",
    "    print(\"Getting Your files...\")\n",
    "    ######################################################\n",
    "    # getting the data from s3\n",
    "    parsed = json.loads(data)\n",
    "    bucket = parsed['bucket']\n",
    "    hocr_file_name = parsed['hocr_file']\n",
    "    image_file_name_s3 = parsed['image_file']\n",
    "\n",
    "    # endpoints\n",
    "\n",
    "    loc_endpoint = parsed.get(\"loc_endpoint\", \"localization-model-2019-01-29\")\n",
    "\n",
    "    hw_endpoint = parsed.get(\"hw_endpoint\", \"pytorch-handwriting-ocr-2019-01-29-02-06-44-538\")\n",
    "    hp_endpoint = parsed.get(\"hp_endpoint\", \"hand-printed-model-2019-01-29-1\")\n",
    "\n",
    "    hw_endpoint_model = parsed.get(\"hw_endpoint_model\", 'new')\n",
    "    hp_endpoint_model = parsed.get(\"hp_endpoint_model\", 'new')\n",
    "\n",
    "    hw_endpoint_new_api = parsed.get(\"hw_endpoint_new_api\", True)\n",
    "    hp_endpoint_new_api = parsed.get(\"hp_endpoint_new_api\", False)\n",
    "\n",
    "    is_new_localizer = parsed.get(\"is_new_localizer\", True)\n",
    "\n",
    "    # access keys\n",
    "    aws_access_key_id = parsed.get(\"aws_access_key_id\", None)\n",
    "    aws_secret_access_key = parsed.get(\"aws_secret_access_key\", None)\n",
    "\n",
    "    # getting data\n",
    "\n",
    "    s3 = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)  # new key\n",
    "    hocr_file_stream = s3.Object(bucket_name=bucket, key=hocr_file_name)\n",
    "    image_file_stream = s3.Object(bucket_name=bucket, key=image_file_name_s3)\n",
    "\n",
    "    temp_hocr_file_name = id_generator() + \".hocr\"\n",
    "    with open(temp_hocr_file_name, 'wb') as f:  #\n",
    "        f.write(hocr_file_stream.get()[\"Body\"].read())\n",
    "\n",
    "    temp_image_file_name = os.path.split(image_file_name_s3)[1]\n",
    "    with open(temp_image_file_name, 'wb') as f:  #\n",
    "        f.write(image_file_stream.get()[\"Body\"].read())\n",
    "\n",
    "        # communicating with endpoints\n",
    "    s3 = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "    session = boto3.Session(region_name='us-west-2', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "    sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "    loc_predictor = MXNetPredictor(loc_endpoint, sagemaker_session)\n",
    "\n",
    "    hw_predictor = JSONPredictor(hw_endpoint, sagemaker_session)\n",
    "    hp_predictor = MXNetPredictor(hp_endpoint, sagemaker_session)\n",
    "\n",
    "    print(\"Uploading small image..\")\n",
    "    img_orig = cv2.imread(temp_image_file_name, 0)  # 7948*6198\n",
    "    img_for_predictors = cv2.resize(img_orig.copy(), (2479, 3508))  # will be 3508*2479\n",
    "\n",
    "    tess_scaling_vert = img_orig.shape[0] / img_for_predictors.shape[0]\n",
    "    tess_scaling_hor = img_orig.shape[1] / img_for_predictors.shape[1]\n",
    "    print(\"loc to tess scaling\", (tess_scaling_vert, tess_scaling_hor))\n",
    "\n",
    "    # ex: a/b/c.tiff\n",
    "    directory_for_image = os.path.split(image_file_name_s3)[0]  # dir  a/b\n",
    "    input_image_name_without_ext = os.path.split(image_file_name_s3)[1].split(\".\")[0]  # c image name\n",
    "    image_extension = os.path.split(image_file_name_s3)[1].split(\".\")[1]  # tiff\n",
    "    small_image_name = input_image_name_without_ext + \"-small.\" + image_extension  # c-small.tiff\n",
    "    small_image_path_s3 = os.path.join(directory_for_image, input_image_name_without_ext, small_image_name)\n",
    "\n",
    "    cv2.imwrite(small_image_name, img_for_predictors)\n",
    "\n",
    "    upload_file(s3, bucket, open(small_image_name, \"rb\"), small_image_path_s3)  #\n",
    "\n",
    "    print(\"Calling localizer..\")\n",
    "\n",
    "    print(\"sending to localizer:\" + small_image_path_s3)\n",
    "\n",
    "    if is_new_localizer:\n",
    "        loc_data = {\"url\": \"s3://{}/{}\".format(bucket, small_image_path_s3)}\n",
    "    else:\n",
    "        loc_data = {'bucket': bucket, 'file_name': small_image_path_s3}\n",
    "    # Modify here\n",
    "    tb = None\n",
    "    try:\n",
    "        loc_out = loc_predictor.predict(loc_data)\n",
    "\n",
    "    except Exception as ex:\n",
    "        tb = traceback.format_exc()\n",
    "\n",
    "    if tb is not None:\n",
    "        print(\"ERROR: {0}\".format(tb))\n",
    "        response_body = json.dumps({\"status\": \"ERROR\", \"traceback\": tb, \"data\": data})\n",
    "        return response_body, output_content_type\n",
    "\n",
    "    loc_out = loc_out[\"result\"]\n",
    "    # loc_out = loc_predictor.predict(loc_data)\n",
    "\n",
    "    hw_data = {\"bucket\": loc_out[\"bucket_name\"], \"file_name\": loc_out[\"hw_key\"], \"model\": hw_endpoint_model}\n",
    "    hp_data = {\"bucket\": loc_out[\"bucket_name\"], \"file_name\": loc_out[\"hp_key\"], \"model\": hp_endpoint_model}\n",
    "\n",
    "    img = (255 - copy.copy(img_orig)) / 255\n",
    "    img = img[:, :-100]\n",
    "\n",
    "    print(\"Parsing the document..\")\n",
    "    repository = HocrDocument(temp_hocr_file_name)\n",
    "    # repository.write_equivalent_xml()\n",
    "    pages = repository.parse()\n",
    "    pages_h = repository.hierarchical_parse()\n",
    "\n",
    "    page = pages[list(pages.keys())[0]]\n",
    "    page_h = pages_h[list(pages_h.keys())[0]]\n",
    "\n",
    "    ##########################################################\n",
    "    \n",
    "    print(\"Calling Handprinting OCR...\")\n",
    "    try:\n",
    "        repository.add_from_deployed(list(pages.keys())[0], hp_data, hp_predictor, is_new_api=hp_endpoint_new_api)\n",
    "    except Exception as ex:\n",
    "        tb = traceback.format_exc()\n",
    "    if tb is not None:\n",
    "        print(\"ERROR: {0}\".format(tb))\n",
    "        response_body = json.dumps({\"status\": \"ERROR\", \"traceback\": tb, \"hp_data\": hw_data})\n",
    "        return response_body, output_content_type\n",
    "    print(\"Calling Handwriting OCR...\")\n",
    "\n",
    "    try:\n",
    "        repository.add_from_deployed(list(pages.keys())[0], hw_data, hw_predictor, is_new_api=hw_endpoint_new_api)\n",
    "    except Exception as ex:\n",
    "        tb = traceback.format_exc()\n",
    "    if tb is not None:\n",
    "        print(\"ERROR: {0}\".format(tb))\n",
    "        response_body = json.dumps({\"status\": \"ERROR\", \"traceback\": tb, \"hw_data\": hw_data})\n",
    "        return response_body, output_content_type\n",
    "    #########################################################\n",
    "\n",
    "    def clean_files():\n",
    "        os.remove(small_image_name)\n",
    "        os.remove(temp_hocr_file_name)\n",
    "        os.remove(temp_image_file_name)\n",
    "\n",
    "    hw_thread = threading.Thread(target=clean_files)\n",
    "    hw_thread.start()\n",
    "\n",
    "    # import threading\n",
    "    # print(\"Calling Handwriting OCR...\")\n",
    "    # hw_thread = threading.Thread(target=lambda: repository.add_from_deployed(list(pages.keys())[0], hw_data, hw_predictor))\n",
    "    # hw_thread.start()\n",
    "    #\n",
    "    # print(\"Calling Handprinting OCR...\")\n",
    "    # hp_thread = threading.Thread(target=lambda: repository.add_from_deployed(list(pages.keys())[0], hp_data, hp_predictor))\n",
    "    # hp_thread.start()\n",
    "    #\n",
    "    # hw_thread.join()\n",
    "    # hp_thread.join()\n",
    "    #########################################################\n",
    "\n",
    "    page_scaling_hor = img.shape[1] / page['width']  # pages[1] : page text boxes coordinate system dimensions\n",
    "    page_scaling_vert = img.shape[0] / page['height']  # pages[1] : page text boxes coordinate system dimensions\n",
    "    print(\"Expected image:\" + page[\"image\"], \".......Given:\" + image_file_name_s3)\n",
    "\n",
    "    print(\"Text space from(PDF):\", (page[\"height\"], page[\"width\"]))\n",
    "    print(\"Image space:\", img.shape)\n",
    "    df_tabula = None  # If tables with lines, better use tabula\n",
    "\n",
    "    if (df_tabula == None):\n",
    "        verbose = False\n",
    "        # 3508x2379 ~ 90linesx25words ~ pixels/word = 96, pixels/line=40--> strip_height > 40 (*2 for header usually > 2 lines) (100) w_max_pool < 96 (50)\n",
    "        tables_detector = TableDetector(verbose, strip_height=50, w_max_pool=75, min_col_width=250, ratio_clip_max=0.25)\n",
    "        print(\"Detecting Tables..\")\n",
    "        tables = tables_detector.detect_tables(img)\n",
    "        # tables_detector.visualize_tables(img_orig)\n",
    "        print(\"Fitting Text..\")\n",
    "        tables_detector.fit_bordered_tables()  # <<< pdf coordinates are compared to image coordinates (scaling needed)\n",
    "        tables_detector.layout_based_borderless_detection()\n",
    "        response_body = tables_detector.get_json_response()\n",
    "    else:\n",
    "        df_res = df_tabula\n",
    "        list_of_json_tables = [json.loads(df_res.to_json())]\n",
    "        response_body = json.dumps({\"data\": list_of_json_tables})\n",
    "\n",
    "    print(os.popen(\"df . -m\").read())\n",
    "    return response_body, output_content_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Your files...\n",
      "Uploading small image..\n",
      "loc to tess scaling (3.3985176738882554, 3.7503025413473177)\n",
      "Calling localizer..\n",
      "sending to localizer:Accident Claim - 2_3-large/Accident Claim - 2_3-large-small.tiff\n",
      "Parsing the document..\n",
      "Calling Handprinting OCR...\n",
      "Downloading original pickle from localizer...\n",
      "Uploading refined pickle pickle from localizer to Accident Claim - 2_3-large/Accident Claim - 2_3-large-small/hand_printed.pkl..\n",
      "Calling the predictor..\n",
      "Fuckin : None\n",
      "{'status': 'SUCCESS', 'result': [{'text': '3LLV', 'y': 2066, 'score': 0.9183891571129037, 'w': 75, 'h': 19, 'type of text': 'HP', 'x': 194}, {'text': 'A', 'y': 573, 'score': 0.9605943341739681, 'w': 84, 'h': 19, 'type of text': 'HP', 'x': 150}, {'text': 'P', 'y': 573, 'score': 0.998618739536266, 'w': 28, 'h': 25, 'type of text': 'HP', 'x': 242}, {'text': 'IM', 'y': 573, 'score': 0.9668640577764153, 'w': 51, 'h': 19, 'type of text': 'HP', 'x': 278}, {'text': '', 'y': 573, 'score': 0.9614442830399739, 'w': 19, 'h': 18, 'type of text': 'HP', 'x': 335}, {'text': '08#03#18', 'y': 832, 'score': 0.9893567192576195, 'w': 514, 'h': 80, 'type of text': 'HP', 'x': 1552}, {'text': '05#01#75', 'y': 681, 'score': 0.9867603186532435, 'w': 530, 'h': 76, 'type of text': 'HP', 'x': 1852}, {'text': '05#01#75', 'y': 556, 'score': 0.9675276535657926, 'w': 525, 'h': 79, 'type of text': 'HP', 'x': 1852}, {'text': 'FP', 'y': 573, 'score': 0.940397623578854, 'w': 94, 'h': 18, 'type of text': 'HP', 'x': 389}, {'text': '', 'y': 573, 'score': 0.9935583213102321, 'w': 61, 'h': 24, 'type of text': 'HP', 'x': 491}, {'text': 'M', 'y': 573, 'score': 0.9735921597561756, 'w': 16, 'h': 18, 'type of text': 'HP', 'x': 575}, {'text': 'I', 'y': 573, 'score': 0.9459564809123797, 'w': 34, 'h': 18, 'type of text': 'HP', 'x': 603}, {'text': 'RA', 'y': 573, 'score': 0.936956499565935, 'w': 25, 'h': 18, 'type of text': 'HP', 'x': 690}, {'text': 'N', 'y': 578, 'score': 0.9743826009938189, 'w': 38, 'h': 19, 'type of text': 'HP', 'x': 755}, {'text': 'IP', 'y': 573, 'score': 0.9487015858690424, 'w': 41, 'h': 18, 'type of text': 'HP', 'x': 799}, {'text': 'H', 'y': 573, 'score': 0.963527784017943, 'w': 18, 'h': 18, 'type of text': 'HP', 'x': 847}, {'text': 'HVA L', 'y': 573, 'score': 0.923854332341489, 'w': 78, 'h': 19, 'type of text': 'HP', 'x': 871}, {'text': '', 'y': 573, 'score': 0.9909989466086614, 'w': 105, 'h': 24, 'type of text': 'HP', 'x': 956}, {'text': 'I', 'y': 681, 'score': 0.9508625816831693, 'w': 50, 'h': 23, 'type of text': 'HP', 'x': 160}, {'text': 'I3 L', 'y': 682, 'score': 0.8835863891331704, 'w': 89, 'h': 18, 'type of text': 'HP', 'x': 217}, {'text': 'A', 'y': 681, 'score': 0.9994796248496035, 'w': 25, 'h': 19, 'type of text': 'HP', 'x': 313}, {'text': '1I', 'y': 682, 'score': 0.9491197747451595, 'w': 30, 'h': 18, 'type of text': 'HP', 'x': 343}, {'text': ' LI', 'y': 681, 'score': 0.8782853829613051, 'w': 72, 'h': 19, 'type of text': 'HP', 'x': 379}, {'text': '', 'y': 681, 'score': 0.9791661551554558, 'w': 60, 'h': 19, 'type of text': 'HP', 'x': 453}, {'text': 'IS', 'y': 682, 'score': 0.9227057885166873, 'w': 61, 'h': 18, 'type of text': 'HP', 'x': 521}, {'text': 'I', 'y': 725, 'score': 0.9542671282552451, 'w': 54, 'h': 18, 'type of text': 'HP', 'x': 127}, {'text': 'L', 'y': 725, 'score': 0.9719266618738164, 'w': 17, 'h': 17, 'type of text': 'HP', 'x': 189}, {'text': 'IIW', 'y': 725, 'score': 0.8707178667839717, 'w': 92, 'h': 22, 'type of text': 'HP', 'x': 277}, {'text': 'IM', 'y': 725, 'score': 0.8848545086018008, 'w': 91, 'h': 22, 'type of text': 'HP', 'x': 530}, {'text': 'NLI', 'y': 725, 'score': 0.9156616434383649, 'w': 103, 'h': 22, 'type of text': 'HP', 'x': 630}, {'text': '1LI', 'y': 725, 'score': 0.9061684544378809, 'w': 96, 'h': 18, 'type of text': 'HP', 'x': 923}]}\n",
      "[{'text': '3LLV', 'y': 2066, 'score': 0.9183891571129037, 'w': 75, 'h': 19, 'type of text': 'HP', 'x': 194}, {'text': 'A', 'y': 573, 'score': 0.9605943341739681, 'w': 84, 'h': 19, 'type of text': 'HP', 'x': 150}, {'text': 'P', 'y': 573, 'score': 0.998618739536266, 'w': 28, 'h': 25, 'type of text': 'HP', 'x': 242}, {'text': 'IM', 'y': 573, 'score': 0.9668640577764153, 'w': 51, 'h': 19, 'type of text': 'HP', 'x': 278}, {'text': '', 'y': 573, 'score': 0.9614442830399739, 'w': 19, 'h': 18, 'type of text': 'HP', 'x': 335}, {'text': '08#03#18', 'y': 832, 'score': 0.9893567192576195, 'w': 514, 'h': 80, 'type of text': 'HP', 'x': 1552}, {'text': '05#01#75', 'y': 681, 'score': 0.9867603186532435, 'w': 530, 'h': 76, 'type of text': 'HP', 'x': 1852}, {'text': '05#01#75', 'y': 556, 'score': 0.9675276535657926, 'w': 525, 'h': 79, 'type of text': 'HP', 'x': 1852}, {'text': 'FP', 'y': 573, 'score': 0.940397623578854, 'w': 94, 'h': 18, 'type of text': 'HP', 'x': 389}, {'text': '', 'y': 573, 'score': 0.9935583213102321, 'w': 61, 'h': 24, 'type of text': 'HP', 'x': 491}, {'text': 'M', 'y': 573, 'score': 0.9735921597561756, 'w': 16, 'h': 18, 'type of text': 'HP', 'x': 575}, {'text': 'I', 'y': 573, 'score': 0.9459564809123797, 'w': 34, 'h': 18, 'type of text': 'HP', 'x': 603}, {'text': 'RA', 'y': 573, 'score': 0.936956499565935, 'w': 25, 'h': 18, 'type of text': 'HP', 'x': 690}, {'text': 'N', 'y': 578, 'score': 0.9743826009938189, 'w': 38, 'h': 19, 'type of text': 'HP', 'x': 755}, {'text': 'IP', 'y': 573, 'score': 0.9487015858690424, 'w': 41, 'h': 18, 'type of text': 'HP', 'x': 799}, {'text': 'H', 'y': 573, 'score': 0.963527784017943, 'w': 18, 'h': 18, 'type of text': 'HP', 'x': 847}, {'text': 'HVA L', 'y': 573, 'score': 0.923854332341489, 'w': 78, 'h': 19, 'type of text': 'HP', 'x': 871}, {'text': '', 'y': 573, 'score': 0.9909989466086614, 'w': 105, 'h': 24, 'type of text': 'HP', 'x': 956}, {'text': 'I', 'y': 681, 'score': 0.9508625816831693, 'w': 50, 'h': 23, 'type of text': 'HP', 'x': 160}, {'text': 'I3 L', 'y': 682, 'score': 0.8835863891331704, 'w': 89, 'h': 18, 'type of text': 'HP', 'x': 217}, {'text': 'A', 'y': 681, 'score': 0.9994796248496035, 'w': 25, 'h': 19, 'type of text': 'HP', 'x': 313}, {'text': '1I', 'y': 682, 'score': 0.9491197747451595, 'w': 30, 'h': 18, 'type of text': 'HP', 'x': 343}, {'text': ' LI', 'y': 681, 'score': 0.8782853829613051, 'w': 72, 'h': 19, 'type of text': 'HP', 'x': 379}, {'text': '', 'y': 681, 'score': 0.9791661551554558, 'w': 60, 'h': 19, 'type of text': 'HP', 'x': 453}, {'text': 'IS', 'y': 682, 'score': 0.9227057885166873, 'w': 61, 'h': 18, 'type of text': 'HP', 'x': 521}, {'text': 'I', 'y': 725, 'score': 0.9542671282552451, 'w': 54, 'h': 18, 'type of text': 'HP', 'x': 127}, {'text': 'L', 'y': 725, 'score': 0.9719266618738164, 'w': 17, 'h': 17, 'type of text': 'HP', 'x': 189}, {'text': 'IIW', 'y': 725, 'score': 0.8707178667839717, 'w': 92, 'h': 22, 'type of text': 'HP', 'x': 277}, {'text': 'IM', 'y': 725, 'score': 0.8848545086018008, 'w': 91, 'h': 22, 'type of text': 'HP', 'x': 530}, {'text': 'NLI', 'y': 725, 'score': 0.9156616434383649, 'w': 103, 'h': 22, 'type of text': 'HP', 'x': 630}, {'text': '1LI', 'y': 725, 'score': 0.9061684544378809, 'w': 96, 'h': 18, 'type of text': 'HP', 'x': 923}]\n",
      "Done:Added data from predictor..\n",
      "Calling Handwriting OCR...\n",
      "Downloading original pickle from localizer...\n",
      "Uploading refined pickle pickle from localizer to Accident Claim - 2_3-large/Accident Claim - 2_3-large-small/hand_written.pkl..\n",
      "Calling the predictor..\n",
      "Fuckin : None\n",
      "{'data': '{\"bucket\": \"unum-files\", \"file_name\": \"Accident Claim - 2_3-large/Accident Claim - 2_3-large-small/hand_written.pkl\", \"model\": \"new\"}', 'traceback': 'Traceback (most recent call last):\\n  File \"/opt/ml/code/handwriting-ocr.py\", line 226, in predict_fn\\n    inner_result = predict_fn_inner(data, model)\\n  File \"/opt/ml/code/handwriting-ocr.py\", line 204, in predict_fn_inner\\n    im = cropbw2content(im)\\n  File \"/opt/ml/code/handwriting-ocr.py\", line 84, in cropbw2content\\n    x_start, y_start = min(xs), min(ys)\\nValueError: min() arg is an empty sequence\\n', 'status': 'ERROR'}\n",
      "ERROR: Traceback (most recent call last):\n",
      "  File \"<ipython-input-91-705df0fd2d2a>\", line 1649, in transform_fn\n",
      "    repository.add_from_deployed(list(pages.keys())[0], hw_data, hw_predictor, is_new_api=hw_endpoint_new_api)\n",
      "  File \"<ipython-input-91-705df0fd2d2a>\", line 525, in add_from_deployed\n",
      "    json_predictions = json_predictions[\"result\"]\n",
      "KeyError: 'result'\n",
      "\n",
      "{\n",
      "    \"hw_data\": {\n",
      "        \"bucket\": \"unum-files\",\n",
      "        \"file_name\": \"Accident Claim - 2_3-large/Accident Claim - 2_3-large-small/hand_written.pkl\",\n",
      "        \"model\": \"new\"\n",
      "    },\n",
      "    \"status\": \"ERROR\",\n",
      "    \"traceback\": \"Traceback (most recent call last):\\n  File \\\"<ipython-input-91-705df0fd2d2a>\\\", line 1649, in transform_fn\\n    repository.add_from_deployed(list(pages.keys())[0], hw_data, hw_predictor, is_new_api=hw_endpoint_new_api)\\n  File \\\"<ipython-input-91-705df0fd2d2a>\\\", line 525, in add_from_deployed\\n    json_predictions = json_predictions[\\\"result\\\"]\\nKeyError: 'result'\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(transform_fn(None,\n",
    "                                         json.dumps(\n",
    "                                             {\"bucket\": bucket,\n",
    "        \"hocr_file\": hocr_file_name,\n",
    "        \n",
    "        \"image_file\":\"Accident Claim - 2_3-large.tiff\", #\n",
    "                                              \n",
    "#         \"image_file\":\"Mock Claims 6-v2_3.tiff\",\n",
    "                                              }\n",
    "                                         )\n",
    "                                         , None, None)[0]), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
